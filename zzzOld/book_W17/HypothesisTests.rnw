<<echo=FALSE, cache=FALSE, results='hide'>>=
set_parent('IntroStats.Rnw')
@

\chapter{Hypothesis Tests} \label{chap:HypothesisTests}
\begin{ChapObj}{\boxwidth}
  \textbf{Objectives:}
  \begin{Enumerate}
    \item Describe the relationship between the scientific method and statistical hypothesis testing.
    \item Properly construct statistical hypotheses.
    \item Describe the concept underlying significance testing.
    \item Describe possible errors in statistical decision making.
  \end{Enumerate}
\end{ChapObj}

\minitoc
\newpage

\lettrine{A}{ statistic is an imperfect estimate} of the unknown parameter because of sampling variability.  There are two calculations using the results of a single sample that recognize this imperfectness and allow conclusions to be made about a parameter.  First, a researcher may form an \emph{a priori} hypothesis about the parameter and then use the information in the sample to make a judgment about the ``correctness'' of that hypothesis.  Second, a researcher may form, from the information in the sample, a range of values that is likely to contain the parameter.  The first method is called \emph{hypothesis testing} and is the subject of this module. The second method consists of constructing a \emph{confidence region}, which is introduced in \modref{chap:ConfidenceRegions}. Specific applications of these two techniques are described in Modules \ref{chap:ZTest}-\ref{chap:GOF}.


\section{Hypothesis Testing \& Scientific Method} \label{sect:SciMethod}
\vspace{-12pt}
In its simplest form, the scientific method has four steps:\index{Scientific Method}

\vspace{-12pt}
\begin{Enumerate}
  \item Observation and description of a natural phenomenon.
  \item Formulation of a hypothesis to explain the phenomenon.
  \item Use the hypothesis to predict new observations.
  \item Experimentally test the predictions.
\end{Enumerate}
\vspace{-6pt}

If the results of the experiment do not match the predictions, then the hypothesis is rejected and an alternative hypothesis is proposed.  If the results of the experiment closely match the predictions, then belief in the hypothesis is gained, but the hypothesis will likely be subjected to further scrutiny.

Statistical hypothesis testing is key to using the scientific method in many fields of study and, in fact, closely follows the scientific method in concept.  Statistical hypothesis testing begins by formulat two competing statistical hypotheses from a research hypothesis.  One of these hypotheses (the null) is used to predict a parameter of interest.  Data is then collected and statistical methods are used to determine whether the observed statistic closely matches the prediction made from the null hypothesis or not.  Probability \modrefp{chap:ProbIntro} is used to measure the degree of matching and sampling variability is taken into account.  This process and the theory underlying statistical hypothesis testing is explained in this module.


\section{Statistical Hypotheses} \label{sec:Hypotheses}
\vspace{-12pt}
Hypotheses in a research study are classified into two types: (1) research hypothesis and (2) statistical hypotheses.  A research hypothesis is a ``wordy'' statement about the question or phenomenon that the researcher is testing.\index{Hypothesis!Research}  The research hypothesis is transferred into statistical hypotheses that are mathematical and more easily subjected to statistical methods.

\defn{Research Hypothesis}{A general statement about the question or phenomenon being tested.}

\vspace{-12pt}
\defn{Statistical Hypothesis}{Mathematical statements about the question or phenomenon being tested.}

There are two types of statistical hypotheses: (1) the null hypothesis and (2) the alternative hypothesis.  The \textbf{null hypothesis}, abbreviated as $H_{0}$, is a specific statement of no difference between a parameter and a specific value or between two parameters.\index{Hypothesis!Null}  The $H_{0}$ ALWAYS contains an equals sign because it always represents ``no difference.''  The \textbf{alternative hypothesis}, abbreviated as $H_{A}$, always states that there is some sort of difference between a parameter and a specific value or between two parameters.\index{Hypothesis!Alternative}  The type of difference comes from the research hypothesis and will require use of a less than (\verb"<"), greater than (\verb">"), or not equals ($\neq$) sign.

\defn{Null Hypothesis}{A statistical hypothesis that states specifically that there is no difference between a parameter and a specific value or between two parameters; typically abbreviated with H$_{0}$.}

\vspace{-12pt}
\defn{Alternative Hypothesis}{A statistical hypothesis that states a specific difference between a parameter and a specific value or between two parameters; typically abbreviated with $H_{A}$.}

\vspace{-12pt}
\warn{Null hypotheses always represent the ``no difference'' situation and, thus, always contain an equals sign.}

\vspace{-12pt}
\warn{Alternative hypotheses always represent some sort of difference and, thus, always contain one of these three directional symbols ($\neq$, $>$, and $<$).}

The relationships between the research, null, and alternative hypotheses are illustrated with the following examples:

\begin{Enumerate}
  \item A medical researcher is concerned that a new medicine may change the patients' mean pulse rate (from the ``known'' mean pulse rate of 82 bpm for individuals in the study population not using the new medicine).
  \begin{Itemize}
    \item $H_{A}:\mu\neq82$ and $H_{0}:\mu=82$ (where $\mu$ represents the mean pulse rate for individuals in the study population that take the new medicine; thus, the alternative hypothesis represents a change from the ``normal'' pulse rate).
  \end{Itemize}
   \item A chemist has invented an additive to automobile batteries that she thinks will extend the current 36 month average life of a battery.
  \begin{Itemize}
    \item $H_{A}:\mu>36$ and $H_{0}:\mu=36$ (where $\mu$ represents mean life of batteries with the new additive; thus, this alternative hypothesis represents an extension of the current battery life).
  \end{Itemize}
  \item An engineer wants to determine if a new type of insulation will reduce the average heating costs of a typical house (which are currently \$145 per month).
   \begin{Itemize}
    \item $H_{A}:\mu<145$ and $H_{0}:\mu=145$ (where $\mu$ represents the mean monthly heating bill for houses that receive the new type of insulation; thus, this alternative hypothesis represents a decline in heating bills from the previous ``normal'' amount).
  \end{Itemize}
\end{Enumerate}

The sign used in the alternative hypothesis comes directly from the wording of the research hypothesis \tabrefp{tab:HAwords}.  An alternative hypothesis that contains the $\neq$ sign is called a \textbf{two-tailed alternative}, as the value can be ``not equal'' to another value in two ways; i.e., less than or greater than.  Alternative hypotheses with the $<$ or the $>$ signs are called \textbf{one-tailed alternatives}.  The null hypothesis is easily constructed from the alternative hypothesis by replacing the sign in the alternative hypothesis with an equals sign.

\begin{table}[htbp]
  \caption{Common words that indicate which sign to use in the alternative hypothesis.}
  \label{tab:HAwords}
  \centering
  \begin{tabular}{ccc}
\hline\hline
$>$ & $<$ & $\neq$ \\
\hline
is greater than & is less than & is not equal to \\
is more than & is below & is different from \\
is larger than & is lower than & has changed from \\
is longer than & is shorter than & is not the same as \\
is bigger than & is smaller than &  \\
is better than & is reduced from &  \\
is at least & is at most &  \\
is not less than & is not more than &  \\
\hline\hline
  \end{tabular}
\end{table}

\warn{The ``not-equals'' alternative is called a two-tailed alternative, whereas the other two alternative hypotheses are called one-tailed alternatives.}

\newpage
\begin{exsection}
  \item \label{revex:HypTCactus} A researcher is investigating the mean growth of a certain cactus under a variety of environmental conditions. Under the current environmental conditions, he hypothesizes that mean growth is no more than 4 cm. What is $H_{0}$ and $H_{A}$ in this situation? \ansref{ans:HypTCactus}
  \item \label{revex:HypTBodyTemp} \cite{Machowiaketal1992} critically examined the belief that the mean body temperature differed from 98.6$^{0}$F by measuring the body temperatures of 93 healthy humans.  What is $H_{0}$ and $H_{A}$ in this situation? \ansref{ans:HypTBodyTemp}
  \item \label{revex:HypTPain} A study by \cite{Cheshireetal1994} reported on six patients with chronic myofascial pain syndrome. The authors were examining the hypothesis that the mean pain length was greater than 2.5 years.  What is $H_{0}$ and $H_{A}$ in this situation? \ansref{ans:HypTPain}
\end{exsection}


\vspace{-12pt}
\subsection{Hypothesis Testing Concept}\index{Hypothesis Testing!Concept}
\vspace{-12pt}
Statistical hypothesis testing begins by using the null hypothesis to make a prediction of what value one should expect for the mean in a sample.  So, for the Square Lake example (from \modref{chap:WhatStatsImportant}), if $H_{0}:\mu=105$ and $H_{A}:\mu<105$, then one would expect, if the null hypothesis is true, that the observed sample mean would be 105.  If sampling variability did not exist and the observed sample mean was NOT equal to 105, then the prediction based on the null hypothesis would not be supported and the conclusion would be that the null hypothesis is incorrect.  In other words, one would conclude that the population mean was not equal to 105.

Of course, sampling variability does exist and its existence complicates matters.  The simple interpretation of not supporting $H_{0}$ because the observed sample mean did not equal the hypothesized population mean canNOT be made because, with sampling variability, one would not expect a statistic to exactly equal the parameter in the population from which the sample was extracted.  For example, even if the null hypothesis was correct, then one would not expect, with sampling variability, the observed sample mean to exactly equal 105; rather, one would expect the observed sample mean to be \textbf{reasonably} close to 105.

Thus, hypothesis testing is a procedure for determining if the difference between the observed statistic and the expected statistic based on the null hypothesis is ``large'' \textbf{relative to sampling variability}.  For example, the standard error of $\bar{x}$ in samples of $n=50$ in the Square Lake example is $\frac{\sigma}{\sqrt{n}}=$$\frac{31.5}{\sqrt{50}}$$=4.45$.  Thus, with this amount of sampling variability, an observed sample mean of 103 would be considered reasonably close to 105 and one would have more belief in $H_{0}:\mu=105$.  However, an observed sample mean of 70 would be considered further away from 105 than one would expect based on sampling variability alone and the belief in $H_{0}:\mu=105$ would lessen.

While the above procedure is intuitively appealing, it loses some of its objectivity when the examples chosen (i.e., samples means of 103 and 70) are not as extremely close or distant from the null hypothesized value (e.g., what would the conclusion be if the observed sample mean was 97?).  A first step in creating a more objective decision criteria is to compute the ``p-value.''\index{p-value}  A p-value is the probability of the observed statistic or a value of the statistic more extreme assuming that the null hypothesis is true.  The p-value is described in more detail below given its centrality to making conclusions about statistical hypotheses.

\defn{p-value}{The probability of the observed statistic or a value of the statistic more extreme assuming the null hypothesis is true.}

The meaning of the phrase ``or more extreme'' is derived from the sign in $H_{A}$ \figrefp{fig:HOtails}.  If $H_{A}$ is the ``less than'' situation, then ``or more extreme'' means ``less than'' or ``shade to the left'' for the probability calculation.  The ``greater than'' situation is defined similarly but would result in shading to the ``right.''  In the ``not equals'' situation, ``or more extreme'' means further into the tail AND the exact same size of tail on the other side of the distribution.  It is clear from \figref{fig:HOtails} why ``less than'' and ``greater than'' are one-tailed alternatives and ``not equals'' is a two-tailed alternative.

<<HOtails, echo=FALSE, fig.width=6, fig.height=2, out.width='.8\\linewidth', fig.cap="Depiction of ``or more extreme'' (red shaded area) in p-values for the three possible alternative hypotheses.">>=
par(mfcol=c(1,3),mar=c(2,0,2,0),las=1,tcl=-0.2)
x0 <- seq(-4,4,by=0.001)
norm0 <- dnorm(x0,0,1)
plot(x0,norm0,type="l",xlab="",ylab="",axes=FALSE,lwd=3)
mtext(expression(bold(H[A]:mu<mu[0])),3)
cv1 <- -1.5
xc1 <- c(cv1,x0[x0<=cv1],cv1)
yc1 <- c(0,norm0[x0<=cv1],0)
polygon(xc1,yc1,col="red",border="red")
lines(x0,norm0,lwd=3)
text(cv1,-0.03,expression(bar(x)),cex=2,xpd=TRUE)

plot(x0,norm0,type="l",xlab="",ylab="",axes=FALSE,lwd=3)
mtext(expression(bold(H[A]:mu>mu[0])),3)
cv2 <- 1.5
xc2 <- c(cv2,x0[x0>=cv2],cv2)
yc2 <- c(0,norm0[x0>=cv2],0)
polygon(xc2,yc2,col="red",border="red")
lines(x0,norm0,lwd=3)
text(cv2,-0.03,expression(bar(x)),cex=2,xpd=TRUE)

plot(x0,norm0,type="l",xlab="",ylab="",axes=FALSE,lwd=3)
mtext(expression(bold(H[A]:mu!=mu[0])),3)
polygon(xc1,yc1,col="red",border="red")
polygon(xc2,yc2,col="red",border="red")
lines(x0,norm0,lwd=3)
text(cv2,-0.03,expression(bar(x)),cex=2,xpd=TRUE)
@

The ``assuming that the null hypothesis is true'' phrase is used to define a $\mu$ for the sampling distribution on which the p-value will be calculated.  This sampling distribution is called the \textbf{null distribution} because it depends on the value of $\mu$ in the null hypothesis.  One must remember that the null distribution represents the distribution of all possible sample means assuming that the null hypothesis is true; it does NOT represent the actual sample means.\footnote{Of course, unless the null hypothesis happens to be perfectly true.}  The null distribution in the Square Lake example is thus $\bar{x}\sim N(105,4.45)$ because $n=50>30$, $H_{0}:\mu=105$, and SE=$\frac{31.49}{\sqrt{50}}$=$4.45$.

The p-value is computed with a ``forward'' normal distribution calculation on the null sampling distribution.  For example, suppose that a sample mean of 100 was observed with $n=50$ from Square Lake (as it was in \tabref{tab:SquareLakeSample1}).  The p-value in this case would be ``the probability of observing $\bar{x}=100$ or a smaller value assuming that $\mu=105$.''  This probability is computed by finding the area to the left of 100 on a $N(105,4.45)$ null distribution and is the exact same type of calculation that was made in \sectref{sect:sdprob}.  Thus, this p-value of \Sexpr{kPvalue(pnorm(100,mean=105,sd=31.49/sqrt(50)))} is computed below \figrefp{fig:SLpvalue1}.

<<SLpvalue1, par1a=TRUE, fig.cap="Depiction of the p-value for the Square Lake example where $\\bar{x}=100$ and $H_{A}:\\mu<105$.">>=
( distrib(100,mean=105,sd=31.49/sqrt(50)) )
@

Interpreting the p-value requires critically thinking about the p-value definition and how it is calculated.  Small p-values appear when the observed statistic is ``far'' from the value expected from the null hypothesis.  In this case there is a small probability of seeing the observed statistic ASSUMING that $H_{0}$ is true.  Thus, the assumption is likely wrong and $H_{0}$ is likely incorrect.  In contrast, large p-values appear when the observed statistic is close to the null hypothesized value suggesting that the assumption about $H_{0}$ may be correct.

\warn{Small p-values are evidence against the null hypothesis.}

The p-value serves as a numerical measure on which to base a conclusion about $H_{0}$.  To do this objectively requires an objective definition of what it means to be a ``small'' or ``large'' p-value.  Statisticians use a cut-off value, called the rejection criterion and symbolized with $\alpha$, such that p-values less than $\alpha$ are considered small and would result in rejecting $H_{0}$ as a viable hypothesis.\index{alpha@{$\alpha$}}  The value of $\alpha$ is typically small, usually set at $0.05$, although $\alpha=0.01$ and $\alpha=0.10$ are also commonly used.

\defn{$\alpha$}{A predetermined rejection criterion value used in hypothesis testing.  This value sets the ``cutoff'' for determining whether it was reasonable to have seen the observed statistic or not assuming the null hypothesis is true.}

\vspace{-12pt}
\warn{Typical values of $\alpha$ are 0.01, 0.05, and 0.10.}

The choice of $\alpha$ is made by the person conducting the hypothesis test and is based on how much evidence a researcher demands before rejecting $H_{0}$.  Smaller values of $\alpha$ require a larger difference between the observed statistic and the null hypothesized value and, thus, require ``more evidence'' of a difference for the $H_{0}$ to be rejected.  For example, if a rejection of the null hypothesis will be heavily scrutinized by regulatory agencies, then the researcher may want to be very sure before claiming a difference and should then set $\alpha$ at a smaller value, say $\alpha=0.01$.  The actual choice for $\alpha$ MUST be made before collecting any data and canNOT be changed once the data has been collected.  In other words, once the data are in hand, a researcher cannot lower or raise $\alpha$ to achieve a desired outcome regarding $H_{0}$.

\warn{The value of the rejection criterion ($\alpha$) is set by the researcher BEFORE data is collected.}

\vspace{-12pt}
\warn{Set $\alpha$ to lower values to make it more difficult to reject $H_{0}$.}

The null hypothesis in the Square Lake example is not rejected because the calculated p-value (i.e., \Sexpr{kPvalue(pnorm(100,mean=105,sd=31.49/sqrt(50)))}) is larger than any of the common values of $\alpha$.  Thus, the conclusion in this example is that it is possible that the mean of the entire population is equal to 105 and it is not likely that the population mean is less than 105.  In other words, observing a sample mean of 100 is likely to happen based on random sampling variability alone and it is unlikely that the null hypothesized value is incorrect.

\begin{exsection}
  \item \label{revex:HypTCalc1} Compute the p-value and make a decision about $H_{0}$ with the following information -- $\alpha=0.10$, $H_{0}:\mu=10$, $H_{A}:\mu>10$, $\sigma=5$, $n=25$, and $\bar{x}=12.1$. \ansref{ans:HypTCalc1}
  \item \label{revex:HypTCalc2} Compute the p-value and make a decision about $H_{0}$ with the following information -- $\alpha=0.05$, $H_{0}:\mu=50$, $H_{A}:\mu<50$, $\sigma=20$, $n=50$, and $\bar{x}=43.8$. \ansref{ans:HypTCalc2}
  \item \label{revex:HypTCalc3}  Compute the p-value and make a decision about $H_{0}$ with the following information -- $\alpha=0.01$, $H_{0}:\mu=100$, $H_{A}:\mu\neq100$, $\sigma=15$, $n=100$, and $\bar{x}=98$. \ansref{ans:HypTCalc3}
  \item \label{revex:HypTWhy} Describe why we must formally go through the steps of a hypothesis test to conclude that $\mu>11$ when we observe $\bar{x}=12.1$. \ansref{ans:HypTWhy}
\end{exsection}


\section{Test Statistics and Effect Sizes}
\vspace{-12pt}
Instead of reporting the observed statistic and the resulting p-value, it may be of interest to know how ``far'' the observed statistic was from the hypothesized value of the parameter.  This is easily calculated with
\[ \text{Observed Statistic}-\text{Hypothesized Parameter} \]
where ``Hypothesized Parameter'' represents the specific value in $H_{0}$.  However, the meaning of this value is difficult to interpret without an understanding of the standard error of the statistic.  For example, a difference of 10 between the observed statistic and the hypothesized parameter seems ``very different'' if the standard error is 1 but does not seem ``different'' if the standard error is 100.  Thus, it is common practice to standardize this difference by dividing by the standard error of the statistic.  This measure of distance is called a \emph{test statistic} and is generalized with

\begin{equation}  \label{eqn:zTestStatGeneral}
  \text{Test Statistic} = \frac{\text{Observed Statistic}-\text{Hypothesized Parameter}}{SE_{\text{Statistic}}}
\end{equation}

Thus, the test statistic \eqref{eqn:zTestStatGeneral} measures how many standard errors the observed statistic is away from the hypothesized parameter.  Relatively large values are indicative of a difference that is likely not due to randomness (i.e., sampling variability) and suggest rejecting the null hypothesis.  There are other forms for calculating test statistics, but all test statistics retain the general idea of scaling the difference between what was observed and what was expected from the null hypothesis in terms of sampling variability.  Even though there is a one-to-one relationship between a test statistic and a p-value, a test statistic is often reported with a hypothesis test to give another feel for the magnitude of the difference between what was observed and what was predicted.

\warn{A test statistic measures how many standard errors the observed statistic is away from the hypothesized parameter.}


\section{Hypothesis Testing Concept Summary}\index{Hypothesis Testing!Concept}\index{p-value}\index{alpha@{$\alpha$}}
\vspace{-12pt}
In summary, hypotheses are statistically examined with the following procedure.
\begin{Enumerate}
  \item Construct null and alternative hypotheses from the research hypothesis.
  \item Construct an expected value of the statistic based on the null hypothesis (i.e., assume that the null hypothesis is true).
  \item Calculate an observed statistic from the individuals in a sample.
  \item Compare the difference between the observed statistic and the expected statistic based on the null hypothesis in relation to sampling variability (i.e., calculate a test statistic and p-value).
  \item Use the $p-value$ to determine if this difference is ``large'' or not.
  \begin{Itemize}
    \item If this difference is ``large'' (i.e., $p-value<\alpha$), then reject the null hypothesis.
    \item If this difference is not ``large'' (i.e., $p-value>\alpha$), then ``Do Not Reject'' the null hypothesis.
  \end{Itemize}
\end{Enumerate}

Statisticians say ``do not reject H$_{0}$'' rather than ``accept H$_{0}$ as true'' when the p-value $>\alpha$ for two reasons. First, there are several other possible values, besides the specific value in the null hypothesis, that would lead to ``do not reject'' conclusions.  For example, if a null hypothesized value of 105 was not rejected, then values of 104.99, 104.98, etc. would also likely not be rejected\footnote{In fact, for example, the values in a 95\% confidence interval -- see \modref{chap:ConfRegions} -- represent all possible hypothesized values that would not be rejected with a two-tailed $H_{A}$ using $\alpha=0.05$.}  So, we don't say that we ``accept'' a particular hypothesized value when we know many other values would also be ``accepted.''

Second, the null hypothesis is almost always not true.  Consider the null hypothesis of the Square Lake example (i.e., ``that the mean length is 105'').  The mean length of fish in Square Lake is undoubtedly not exactly equal to 105.  It may be 104.9, 105.01, or some other more disparate value.  The point is that the specific value of the hypothesis is likely never true, especially for a continuous variable.  The problem is that it takes large amounts of data to be able to distinguish means that are very close to the true population mean (i.e., it is difficult to distinguish between 104.9 and 105 when sampling variability is present).  Very often we will not take a sample size large enough to distinguish these subtle differences.  Thus, we will say that we ``do not reject H$_{0}$'' because there simply was not enough data to reject it.

\vspace{-18pt}
\begin{exsection}
\vspace{-12pt}
  \item \label{revex:HypTEffluent} The managers of a wastewater treatment plant monitored the amount of biological oxygen demand (BOD; lbs/day) in the effluent of the plant each month from January 1991 to October 2000.  The managers would need to take corrective actions if the average BOD over this time period was significantly greater than 2200 lbs/day at a 10\% rejection level.  Previous studies indicated that the standard deviation was 1200 lbs/day.  Summary statistics from their sample of days is given below.  Use this information to answer the questions below. \ansref{ans:HypTEffluent}
  \begin{Verbatim}
    n  Min. 1st Qu.  Median   Mean  3rd Qu.    Max.
  118   630    1600    2240   2504     3193    6023
  \end{Verbatim}

  \begin{Enumerate}
    \item What are the null and alternative hypotheses?
    \item What is the test statistic?
    \item \rhw{} Compute the p-value.
    \item Use the p-value to make a decision about $H_{0}$.
    \item What does this mean for the managers of the plant (i.e., will they need to take action)? Explain!
  \end{Enumerate}

  \item \label{revex:HypTMedSchool} Admissions representatives at the University of Minnesota medical school were concerned that the average grade point average of applicants in non-science courses had dropped below 3.7.  A sample of 40 of the most recent applicants indicated that the mean was 3.60.  Information from the Association of American Medical Colleges suggested that the overall standard deviation was 0.35.  Use this information, and an $\alpha=0.05$, to answer the questions below. \ansref{ans:HypTMedSchool}
  \begin{Enumerate}
    \item What are the null and alternative hypotheses?
    \item What is the test statistic?
    \item \rhw{} Compute the p-value.
    \item Use the p-value to make a decision about $H_{0}$.
    \item Was the representatives concern about the average gpa of applicants warranted?  Explain!
  \end{Enumerate}

\end{exsection}


\section{Errors and Power}\index{Hypothesis Testing!Errors}\index{Power}
The goal of hypothesis testing is to make a decision about $H_{0}$.  Unfortunately, because of sampling variability, there is always a risk of making an incorrect decision.  Two types of incorrect decisions can be made \tabrefp{tab:DMerrs}.  A Type I error occurs when a true $H_{0}$ is falsely rejected.  In other words, even if $H_{0}$ is true, there is a chance that a rare sample will occur and $H_{0}$ will be deemed incorrect.  The probability of making a Type I error is set when $\alpha$ is chosen.\index{alpha@{$\alpha$}}  A Type II error occurs when a false $H_{0}$ is not rejected.  The probability of a Type II error is denoted by $\beta$.\index{beta@{$\beta$}}

\begin{table}[htbp]
  \caption{Types of decisions that can be made from a hypothesis test.}
  \label{tab:DMerrs}
  \centering
  \begin{tabular}{cc|c|c|}
    \multicolumn{1}{c}{\widen{-2}{7}{}} & \multicolumn{1}{c}{} & \multicolumn{2}{c}{Decision from Data} \\
    \cline{3-4}
    \multicolumn{1}{c}{\widen{-2}{7}{}} & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{Reject} & \multicolumn{1}{c|}{Not Reject} \\
    \cline{2-4}
    \multicolumn{1}{c|}{\widen{-2}{7}{Truth About}} & \multicolumn{1}{c|}{$H_{0}$} & \multicolumn{1}{c|}{Type I} & \multicolumn{1}{c|}{Correct} \\
    \cline{2-4}
    \multicolumn{1}{c|}{\widen{-2}{7}{Population}} & \multicolumn{1}{c|}{$H_{A}$} & \multicolumn{1}{c|}{Correct} & \multicolumn{1}{c|}{Type II} \\
    \cline{2-4}
  \end{tabular}
\end{table}

\defn{Type I error}{Rejecting $H_{0}$ when $H_{0}$ was actually true.  Probability of Type I error is $\alpha$.}

\vspace{-12pt}
\defn{Type II error}{Not rejecting $H_{0}$ when $H_{0}$ was actually false.  Probability of Type II error is $\beta$.}

The decision in the Square Lake example above produced a Type II error because $H_{0}:\mu=105$ was not rejected even though we know that $\mu=98.06$ \tabrefp{tab:SquareLakePopn}.  Unfortunately, in real life, it will never be known exactly when a Type I or a Type II error has been made because the true $\mu$ is not known.  However, it is known that a Type I error will be made $100\alpha$\% of the time.  The probability of a type II error ($\beta$), though, is never known because this probability depends on the true $\mu$.  Decisions can be made, however, that affect the magnitude of $\beta$ (discussed below with power).

A concept that is very closely related to decision-making errors is the idea of \textbf{power}.\index{Power}  Power is the probability of correctly rejecting a false $H_{0}$.  In other words, it is the probability of detecting a difference from the hypothesized value if a difference really exists.  Power is used to demonstrate how sensitive a hypothesis test is for identifying a difference.  High power related to a $H_{0}$ that is not rejected implies that the $H_{0}$ really should not have been rejected.  Conversely, low power related to a $H_{0}$ that was not rejected implies that the test was very unlikely to detect a difference, so not rejecting $H_{0}$ is not surprising nor particularly conclusive.

\defn{Power}{The probability of correctly rejecting $H_{0}$ when $H_{0}$ was actually false.}

Power is equal to $1-\beta$ and, thus, like $\beta$ it cannot be computed directly.  However, a researcher can make decisions that will positively affect power \figrefp{fig:SLPowerRelations}.  For example, a researcher can positively impact power by increasing $\alpha$ or $n$.  Increasing $n$ is more beneficial because it does not result in an increase in Type I errors as would occur with increasing $\alpha$.  In addition, power decreases as the difference between the hypothesized mean ($\mu_{0}$) and the actual mean ($\mu_{A}$) decreases.  This means that the ability to detect increasingly smaller differences decreases.  In addition, power decreases with an increasing amount of natural variability (i.e., $\sigma$).  In other words, the ability to detect a difference decreases with increasing amounts of variability among individuals.  A researcher cannot control the difference between $\mu_{0}$ and $\mu_{A}$ or the value of $\sigma$.  However, it is important to know that if a situation with a ``large'' amount of variability is encountered or the difference to be detected is small, the researcher will need to increase $n$ to gain power.

\warn{Power = 1-$\beta$.}

\vspace{-12pt}
\warn{Power will increase as the difference between the actual and hypothesized value of the parameter increases.}

\vspace{-12pt}
\warn{Power will increase as the standard error of the statistic decreases.  Thus, power increases as the sample size increases.}

\vspace{-12pt}
\warn{Power will increase as the $\alpha$ level increases.}

<<SLPowerRelations, echo=FALSE, fig.width=7, fig.height=7, out.width='.6\\linewidth', fig.cap="The relationship between one-tailed (lower) power and $\\alpha$, $n$, actual mean ($\\mu_{A}$), and $\\sigma$.  In all situations where the variable does not vary, $\\mu_{0}=105$, $\\mu_{A}=98.06$, $\\sigma=31.49$, $n=50$, and $\\alpha=0.05$. ">>=
par(mfcol=c(2,2),mar=c(3.5,3.5,0.5,0.5),mgp=c(2.1,0.4,0),tcl=-0.2,cex.lab=1.5,las=1)
power <- function(mu0,mua,sigma,n,alpha,uptail=FALSE){
  SE <- sigma/sqrt(n)
  if (uptail) 1-pnorm(qnorm(1-alpha,mu0,SE),mua,SE)
  else pnorm(qnorm(alpha,mu0,SE),mua,SE)
}

# Set the Square Lake values
mu0 <- 105
muA <- 98.06
sigma <- 31.49
n <- 50
alpha <- 0.05
ylmts <- c(0,1)
# Cycle through alphas
alphas <- seq(0.002,0.1,by=0.002)
palpha <- power(mu0,muA,sigma,n,alphas,FALSE)
plot(alphas,palpha,type="l",xlab=expression(alpha),ylab="Power",
     lwd=3,xlim=c(0,0.1),ylim=ylmts,cex=1.5)
# Cycle through muas
muas <- seq(85,105,by=1)
pmua <- power(mu0,muas,sigma,n,alpha,FALSE)
plot(muas,pmua,type="l",xlab=expression(mu[A]),ylab="Power",
     lwd=3,ylim=ylmts)
# Cycle through sample sizes
ns <- seq(2,250)
pn <- power(mu0,muA,sigma,ns,alpha,FALSE)
plot(ns,pn,type="l",xlab="n",ylab="Power",lwd=3,xlim=c(0,max(ns)),ylim=ylmts)
# Cycle through sigmas
sigmas <- seq(1,60,by=0.5)
psigma <- power(mu0,muA,sigmas,n,alpha,FALSE)
plot(sigmas,psigma,type="l",xlab=expression(sigma),ylab="Power",lwd=3,xlim=c(0,max(sigmas)),ylim=ylmts)
@

\vspace{12pt}
Power cannot usually be calculated because the actual mean ($\mu_{A}$) is not known.  However, in the Square Lake example, $\mu_{A}$ is known and power can be calculated in four steps:
\begin{Enumerate}
 \item Draw the sampling distribution assuming the $H_{0}$ is true (called the null distribution).
 \begin{itemize}
   \item The null distribution is $N(105,\frac{31.49}{\sqrt{50}})$ because $H_{0}:\mu=105$, $\sigma=31.49$, and $n=50$.
 \end{itemize}
 \item Find the rejection region borders (based on $\alpha$ and $H_{A}$) in terms of the value of the statistic (a ``reverse'' calculation on the null distribution).
 \begin{itemize}
   \item The rejection region is delineated by the $\bar{x}$ that has $\alpha=0.10$ to the left (because $H_{A}$ is a ``less than'').  The Z with 0.10 to the left of it is -1.282.  Thus, the $\bar{x}$ on the null distribution with 0.10 to the left of it is $-1.282\frac{31.49}{\sqrt{50}}+105 = 99.2908$.
 \end{itemize}
 \item Draw the sampling distribution corresponding to the ``actual'' parameter value (SE is the same as that for the null distribution).
 \begin{itemize}
   \item The actual $\mu$ is 98.06.  Thus, the actual sampling distribution is $N(98.06,\frac{31.49}{\sqrt{50}})$.
 \end{itemize}
 \item Compute the portion of the ``actual'' sampling distribution in the REJECTION region of the null distribution (i.e., a ``forward'' calculation on the actual distribution).
 \begin{itemize}
   \item This computation is to find the area to the left of 99.2908 on $N(98.06,\frac{31.49}{\sqrt{50}})$.  The corresponding Z is $\frac{99.2908-98.06}{\frac{31.49}{\sqrt{50}}}=0.29$.  The area to the left of this Z is 0.6141.
 \end{itemize}
\end{Enumerate}

Thus, the power to detect a $\mu_{A}=98.06$ was 0.6141.  This means that in only about 61\% of the samples will the false $H_{0}:\mu=105$ be correctly rejected.  Thus, it is not too surprising that $H_{0}$ was not rejected in this example.  If $n$ could be doubled to 100, however, the power to correctly reject $H_{0}:\mu=105$ would increase to approximately 0.82 \figrefp{fig:SLPowerRelations}.

\begin{exsection}
  \item \label{revex:HypTPower1} What is $\beta$ if power=0.875? \ansref{ans:HypTPower1}
  \item \label{revex:HypTPowerAlpha} For a constant sample size, $\sigma$, and difference between the hypothesized and actual means, what happens to power, if $\alpha$ is increased?  \ansref{ans:HypTPowerAlpha}
  \item \label{revex:HypTPowern} For a constant $\alpha$, $\sigma$, and difference between the hypothesized and actual means, what happens to power, if the sample size increases? \ansref{ans:HypTPowern}
  \item \label{revex:HypTPowerMu} For a constant $\alpha$, $\sigma$, and sample size, what happens to power if the difference between the hypothesized and actual means increases? \ansref{ans:HypTPowerMu}
  \item \label{revex:HypTBetaAlpha} For a constant sample size, $\sigma$, and difference between the hypothesized and actual means, what happens to $\beta$, if $\alpha$ is increased? \ansref{ans:HypTBetaAlpha}
  \item \label{revex:HypTBetan} For a constant $\alpha$, $\sigma$, and difference between the hypothesized and actual means, what happens to $\beta$, if the sample size is increased? \ansref{ans:HypTBetan}
  \item \label{revex:HypTBetaMu} For a constant $\alpha$, $\sigma$, and sample size, what happens to $\beta$ if the difference between the hypothesized and actual means increases? \ansref{ans:HypTBetaMu}
  \item \label{revex:HypTRealLife} Describe a real-life situation where you think that making a Type II error would be much more ``costly'' than making a Type I error.  Completely describe the situation at hand and what Type I and a Type II errors mean in terms of the situation you describe. \ansref{ans:HypTRealLife}
  \item \label{revex:HypTCalcPwr1} Compute power given the following information: $\alpha=0.05$, $H_{0}:\mu=50$, $H_{A}:\mu<50$, $\sigma=20$, $n=50$, and $\mu_{A}=45$. \ansref{ans:HypTCalcPwr1}
  \item \label{revex:HypTCalcPwr2} Compute power given the following information: $\alpha=0.10$, $H_{0}:\mu=10$, $H_{A}:\mu>10$, $\sigma=5$, $n=25$, and $\mu_{A}=12$. \ansref{ans:HypTCalcPwr2}
  \item \label{revex:HypTCalcPwr3} Compute power given the following information: $\alpha=0.01$, $H_{0}:\mu=75$, $H_{A}:\mu>75$, $\sigma=15$, $n=30$, and $\mu_{A}=82$. \ansref{ans:HypTCalcPwr3}
\end{exsection}
