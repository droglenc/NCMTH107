<<echo=FALSE, cache=FALSE, results='hide'>>=
set_parent('IntroStats.Rnw')
@

\chapter{Linear Regression}  \label{chap:Regress}

\minitoc
\vspace{48pt}

\lettrine{L}{inear regression analysis is used to model the relationship} between two quantitative variables for two related purposes -- (i) explaining variability in the response variable and (ii) predicting future values of the response variable. Examples include predicting future sales of a product from its price, family expenditures on recreation from family income, an animal's food consumption in relation to ambient temperature, and a person's score on a German assessment test based on how many years the person studied German.

Exact predictions cannot be made because of natural variability. For example, two people with the same intake of mercury (from consumption of fish) will not have the same level of mercury in their blood stream (e.g., observe the two individuals in \figref{fig:HGscat} that had intakes of 580 ug HG/day). Thus, the best that can be accomplished is to predict the average or expected value for a person with a particular intake value. This is accomplished by finding the line that best ``fits'' the points on a scatterplot of the data and using that line to make predictions. Finding and using the ``best-fit'' line is the topic of this module.

<<HGscat, echo=FALSE, fig.cap="Scatterplot of intake of mercury in fish and the mercury in the blood stream. The two individuals mentioned in the main text are shown as red points.">>=
merc <- data.frame(intake=c(180,200,230,410,600,550,275,580,580,105,250,60,650),
                   blood=c(90,120,125,290,310,290,170,275,350,70,105,75,480))
plot(blood~intake,data=merc,pch=21,bg="gray70",
     xlab="Intake (ug Hg/day)",ylab="Blood (ng/g)")
points(blood~intake,data=filterD(merc,intake==580),pch=21,bg="red")
axis(1,at=c(200,400,600))
@


\section{Response and Explanatory Variables}
Recall from \sectref{sect:RespExplan1} that the response (or dependent) variable is the variable to be predicted or explained and the explanatory (or independent) variable is the variable that will help do the predicting or explaining. In the examples mentioned above, future sales, family expenditures on recreation, the animal's food consumption, and score on the assessment test are response variables and product price, family income, temperature, and years studying German are explanatory variables, respectively. The response variable is on the y-axis and the explanatory variable is on the x-axis of scatterplots.


\section{Slope and Intercept}
The equation of a line is commonly expressed as,
  \[ y = mx + b  \]
where both $x$ and $y$ are variables, $m$ represents the slope of the line, and $b$ represents the y-intercept.\footnote{Hereafter, simply called the ``intercept.''}  It is important that you can look at the equation of a line and identify the response variable, explanatory variable, slope, and intercept. The response variable will always appear on one side of the equation (usually the left) by itself.  The value or symbol that is multiplied by the explanatory variable (e.g., $x$) is the slope, and the value or symbol by itself is the intercept. For example, in
\[ blood = 3.501 + 0.579*intake \]
\var{blood} is the response variable, \var{intake} is the explanatory variable, $0.579$ is the slope (it is multiplied by the explanatory variable), and $3.501$ is the intercept (it is not multiplied by anything in the equation). The same conclusions would be made if the equation had been written as
  \[ blood = 0.579*intake+3.501 \]

\warn{In the equation of a line, the slope is always multiplied by the explanatory variable and the intercept is always by itself.}

In addition to being able to identify the slope and intercept of a line you also need to be able to interpret these values. Most students define the slope as ``rise over run'' and the intercept as ``where the line crosses the y-axis.''  These ``definitions'' are loose geometric representations. For our purposes, the slope and intercept must be more strictly defined.

To define the slope, first think of ``plugging'' two values of intake into the equation discussed above. For example, if $intake=100$, then $blood=3.501+0.579*100$=\Sexpr{formatC(3.501+0.579*100,format="f",digits=2)} and if $intake$ is one unit larger at $101$), then $blood=3.501+0.579*101$=\Sexpr{formatC(3.501+0.579*101,format="f",digits=2)}.\footnote{For simplicity of exposition, the actual units are not used in this discussion. However, ``units'' would usually be replaced with the actual units used for the measurements.} The difference between these two values is \Sexpr{formatC(3.501+0.579*101,format="f",digits=2)}-\Sexpr{formatC(3.501+0.579*100,format="f",digits=2)}=$0.579$. Thus, the slope is the change in value of the response variable for a single unit change in the value of the explanatory variable \figrefp{fig:SlopeInt}. That is, mercury in the blood changes 0.579 units for a single unit change in mercury intake. So, if an individual increases mercury intake by one unit, then mercury in the blood will increase by 0.579 units, ON AVERAGE. Alternatively, if one individual has one more unit of mercury intake than another individual, then the first individual will have, ON AVERAGE, 0.579 more units of mercury in the blood.

<<SlopeInt, echo=FALSE, fig.cap="Schematic representation of the meaning of the intercept and slope in a linear equation.">>=
par(mar=c(4.5,4.5,0,0),mgp=c(2,0.75,0))
int <- 1.5
slope <- 0.9
plot(-10,-10,xlim=c(0,10),ylim=c(0,10),xaxt="n",yaxt="n",xaxs="i",yaxs="i",xlab="Explanatory Variable",ylab="Response Variable")
axis(1,at=0,col="red")
axis(2,at=int,"",las=1,col="red")
text(0,int,"intercept",col="red",pos=2,xpd=NA)
abline(a=int,b=slope,lwd=3,col="black")
lines(c(2,3,3),int+slope*c(2,2,3),lwd=2,col="red")
text(2.5,int+slope*2,"1",pos=1,col="red")
text(3,int+slope*2.5,"slope",pos=4,col="red")
lines(c(6,7,7),int+slope*c(6,6,7),lwd=2,col="red")
text(6.5,int+slope*6,"1",pos=1,col="red")
text(7,int+slope*6.5,"slope",pos=4,col="red")
@

To define the intercept, first ``plug'' $intake=0$ into the equation discussed above; i.e., $blood=3.501+0.579*0$ = $3.501$. Thus, the intercept is the value of the response variable when the explanatory variable is equal to zero \figrefp{fig:SlopeInt}. In this example, the AVERAGE mercury in the blood for an individual with no mercury intake is 3.501. Many times, as is true with this example, the interpretation of the intercept will be nonsensical. This is because $x=0$ will likely be outside the range of the data collected and, perhaps, outside the range of possible data that could be collected.

The equation of the line is a model for the relationship depicted in a scatterplot. Thus, the interpretations for the slope and intercept represent the \textit{average} change or the \textit{average} response variable. Thus, whenever a slope or intercept is being interpreted it must be noted that the result is an \textit{average} or \textit{on average}.


\section{Predictions}
Once a best-fit line has been identified (criteria for doing so is discussed in \sectref{sect:BestFitLine}), the equation of the line can be used to predict the average value of the response variable for individuals with a particular value of the explanatory variable. For example, the best-fit line for the mercury data shown in \figref{fig:HGscat} is
  \[ blood = 3.501 + 0.579*intake \]
Thus, the predicated average level of mercury in the blood for an individual that consumed 240 ug HG/day is found with
  \[ blood = 3.501 + 0.579*240 = 142.461 \]
Similarly, the predicted average level of mercury in the blood for an individual that consumed 575 ug HG/day is found with
  \[ blood = 3.501 + 0.579*575 = 336.426 \]
A prediction may be visualized by finding the value of the explanatory variable on the x-axis, drawing a vertical line until the best-fit line is reached, and then drawing a horizontal line over to the y-axis where the value of the response variable is read \figrefp{fig:HGpredict}.

<<HGpredict, echo=FALSE, fig.cap="Scatterplot between the intake of mercury in fish and the mercury in the blood stream of individuals with superimposed best-fit regression line illustrating predictions for two values of mercury intake.">>=
par(mar=c(4,4,0.5,0.5),mgp=c(2.5,0.75,0))
plot(blood~intake,data=merc,pch=21,bg="gray70",
     xlab="Intake (ug Hg/day)",ylab="Blood (ng/g)")
abline(a=-3.501,b=0.579,col="red",lwd=2)
df1 <- c(240,575)
p1 <- 3.501+0.579*df1
text(df1[1],30,df1[1],xpd=TRUE,col="red")
text(df1[2],30,df1[2],xpd=TRUE,col="red")
lines(c(df1[1],df1[1],0),c(0,p1[1],p1[1]),col="red",lwd=2,lty=3)
lines(c(df1[2],df1[2],0),c(0,p1[2],p1[2]),col="red",lwd=2,lty=3)
text(-20,p1,formatC(p1,format="f",digits=1),col="red",xpd=TRUE)
@

When predicting values of the response variable, it is important to not extrapolate beyond the range of the data. In other words, predictions with values outside the range of observed values of the explanatory variable should be made cautiously (if at all). An excellent example would be to consider height ``data'' collected during the early parts of a human's life (say the first ten years). During these early years there is likely a good fit between height (the response variable) and age. However, using this relationship to predict an individual's height at age 40 would likely result in a ridiculous answer (e.g., over ten feet). The problem here is that the linear relationship only holds for the observed data (i.e., the first ten years of life); it is not known if the same linear relationship exists outside that range of years. In fact, with human heights, it is generally known that growth first slows, eventually quits, and may, at very old ages, actually decline. Thus, the linear relationship found early in life does not hold for later years. Critical mistakes can be made when using a linear relationship to extrapolate outside the range of the data.

\section{Residuals}
The predicted value is a ``best-guess'' for an individual based on the best-fit line. The actual value for any individual is likely to be different from this predicted value. The \textbf{residual} is a measure of how ``far off'' the prediction is from what is actually observed. Specifically, the residual for an individual is found by subtracting the predicted value (given the individual's observed value of the explanatory variable) from the individual's observed value of the response variable, or
  \[ \text{residual}=\text{observed response}-\text{predicted response} \]

For example, consider an individual that has an observed intake of 650 and an observed level of mercury in the blood of 480. As shown in the previous section, the predicted level of mercury in the blood for this individual is
  \[ blood = 3.501 + 0.579*650 = 379.851 \]

The residual for this individual is then $480-379.851$ = $100.149$. This positive residual indicates that the observed value is approximately 100 units greater than the average for individuals with an intake of 650.\footnote{In other words, the observed value is ``above'' the line.}  As a second example, consider an individual with an observed intake of 250 and an observed level of mercury in the blood of 105. The predicted value for this individual is
  \[ blood = 3.501 + 0.579*250 = 148.251 \]

and the residual is $105-148.251$ = $-43.251$. This negative residual indicates that the observed value is approximately 43 units less than the average for individuals with an intake of 250.

Visually, a residual is the vertical distance between an individual's point and the best-fit line \figrefp{fig:HGresidual}.

<<HGresidual, echo=FALSE, fig.cap="Scatterplot between the intake of mercury in fish and the mercury in the blood stream of individuals with superimposed best-fit regression line illustrating the residuals for the two individuals discussed in the main text.">>=
plot(blood~intake,data=merc,pch=21,bg="gray70",
     xlab="Intake (ug Hg/day)",ylab="Blood (ng/g)")
lm1 <- lm(blood~intake,data=merc)
abline(lm1,col="red",lwd=2)
df2 <- data.frame(intake=c(250,650))
p2 <- predict(lm1,df2)
lines(c(df2[1,1],df2[1,1]),c(105,p2[1]),lwd=2,lty=2,col="blue")
lines(c(df2[2,1],df2[2,1]),c(480,p2[2]),lwd=2,lty=2,col="red")
@

\section{Best-fit Criteria}\label{sect:BestFitLine}
An infinite number of lines can be placed on a graph, but many of those lines do not adequately describe the data. In contrast, many of the lines will appear, to our eye, to adequately describe the data. So, how does one find THE best-fit line from all possible lines. The \textbf{least-squares} method described below provides a quantifiable and objective measure of which line best ``fits'' the data.

Residuals are a measure of how far an individual is from a candidate best-fit line. Residuals computed from all individuals in a data set measure how far all individuals are from the candidate best-fit line. Thus, the residuals for all individuals can be used to identify the best-fit line.

The residual sum-of-squares (RSS) is the sum of all squared residuals. The least-squares criterion says that the ``best-fit'' line is the one line out of all possible lines that has the minimum RSS \figrefp{fig:RSSanim}.

<<RSSanim, echo=FALSE, cache=TRUE, fig.width=7, fig.height=3.5, fig.show='animate',fig.cap="An animation illustrating how the residual sum-of-squares (RSS) for a series of candidate lines (red lines) is minimized at the best-fit line (green line).", aniopts='controls,palindrome,autoplay',out.width='.8\\linewidth'>>=
################################################################################
# Function to calculate the RSS given a dataframe that must have the explanatory
#   variable in the first column and the response variable in the second column
#   and a dataframe that contains the paired parameter estimates with the slopes
#   in the first column and the intercepts in the second column. Returns a data
#   frame with the slopes as the first, the intercepts as the second, and the
#   RSS as the third column.
################################################################################
RSS2 <- function(d,ests) {
  RSS <- NULL                               # Initiate the vector
  for (i in 1:nrow(ests)) {                 # Cycle throught the possible parameter estimates
    pred <- ests[i,1]*d[,1]+ests[i,2]       # Predicted values for current parameter estimates
    RSS <- c(RSS,sum((d[,2]-pred)^2))       # Put the RSS for the current parameter estimates onto the end of the vector
  }
  data.frame(ests,RSS)
}

################################################################################
# A function that receives a dataframe that has the sequence of parameter
#   estimates in the first two columns and the corresponding RSS in the third
#   column. In addition, the index position of the current potential parameters
#   must be sent. This function then plots the RSS (e.g., RSS vs. an index)
#   and a red vertical line and dot at the current estimate.
################################################################################
plotRSS<-function(r,icurr) {
  # Creates an index
  i <- seq(1,nrow(r))
  # Plot the function
  plot(i,r[,3],type="l",lwd=2,main="",xlab="",ylab="RSS",xaxt="n",yaxt="n")
  # Labels x-axis
  mtext(paste("slope","intercept",sep=","),1,1)
  # Put line at current guess
  lines(c(icurr,icurr),c(min(r[,3]),max(r[,3])),lwd=2,col="red")
  # Put point at current guess
  points(icurr,r[icurr,3],pch=21,col="red",bg="gray70",cex=1.2)
}

################################################################################
# Plots the raw data with residuals to the current estimate
################################################################################
plotEst<-function(d,e,icurr,...) {
  # Plot the data
  plot(d[,1],d[,2],xlab=names(d)[1],ylab=names(d)[2],pch=21,bg="gray70",...)
  # Fit the linear model
  l1<-lm(d[,2]~d[,1])
  # Plot the best-fit line in green
  abline(coef(l1),lwd=1,col="green")
  # Plot current candidate line in red
  abline(c(e[icurr,2],e[icurr,1]),lwd=2,col="red")
  # Put on the residuals to the current estimate
  for (i in 1:nrow(d)) {
    pred <- e[icurr,1]*d[i,1]+e[icurr,2]
    clr <- ifelse((d[i,2]-pred)<0,"blue","red")
    lines(c(d[i,1],d[i,1]),c(d[i,2],pred),lty=2,lwd=2,col=clr)
  }
}

# create intercepts
ints <- seq(-180,190,5)
# create slopes from the intercepts given the mean of x and mean of y
xbar <- mean(merc$intake)
ybar <- mean(merc$blood)
slps <- sapply(ints,FUN=function(int,xbar,ybar) (ybar-int)/xbar, xbar=xbar,ybar=ybar)
params <- data.frame(slps,ints)


par(mar=c(3.5,3.5,1,1),mgp=c(2,0.75,0),mfrow=c(1,2))
# Compute RSS values for all parameter choices
RSSs <- RSS2(merc,params)
# Cycle through the estimates
for(i in 1:nrow(params)) {
  # Make the fitted plot
  plotEst(merc,params,i,ylim=c(-100,600),xlim=c(0,700))
  # Make the RSS plot
  plotRSS(RSSs,i)
}
@

The discussion thusfar implies that all possible lines must be ``fit'' to the data and the one with the minimum RSS is chosen as the ``best-fit'' line. As there are an infinite number of possible lines, this would be impossible to do. Theoretical statisticians have shown that the application of the least-squares criterion always produces a best-fit line with a slope given by
  \[ slope = r\frac{s_{y}}{s_{x}}  \]

and an intercept given by
  \[ intercept = \bar{y}-slope*\bar{x}   \]

where $\bar{x}$ and $s_{x}$ are the sample mean and standard deviation of the explanatory variable, $\bar{y}$ and $s_{y}$ are the sample mean and standard deviation of the response variable, and $r$ is the sample correlation coefficient between the two variables. Thus, using these formulas finds the slope and intercept for the line, out of all possible lines, that minimizes the RSS.


\section{Assumptions}\label{sect:RegAssumptions}
The least-squares method for finding the best-fit line only works appropriately if each of the following five assumptions about the data has been met.

\begin{Enumerate}
  \item A line describes the data (i.e., a linear form).
  \item Homoscedasticity.
  \item Normally distributed residuals at a given x.
  \item Independent residuals at a given x.
  \item The explanatory variable is measured without error.
\end{Enumerate}

While all five assumptions of linear regression are important, only the first two are vital when the best-fit line is being used primarily as a descriptive model for data.\footnote{In contrast to using the model to make inferences about a population model.}  Description is the primary goal of linear regression used in this course and, thus, only the first two assumptions are considered further.

The linearity assumption appears obvious -- if a line does not represent the data, then don't try to fit a line to it!  Violations of this assumption are evident by a non-linear or curving form in the scatterplot.

The homoscedasticity assumption states that the variability about the line is the same for all values of the explanatory variable. In other words, the dispersion of the data around the line must be the same along the entire line. Violations of this assumption generally present as a ``funnel-shaped'' dispersion of points from left-to-right on a scatterplot.

Violations of these assumptions are often evident on a ``fitted-line plot'', which is a scatterplot with the best-fit line superimposed \figrefp{fig:ResidPlotEx}.\footnote{Residual plots, not discussed in this text, are another plot that often times is used to better assess assumption violations.} If the points look more-or-less like random scatter around the best-fit line, then neither the linearity nor the homoscedasticity assumption has been violated. A violation of one of these assumptions should be obvious on the scatterplot. In other words, there should be a clear curvature or funneling on the plot.

<<ResidPlotEx, echo=FALSE, fig.width=6, fig.height=6, out.width='.8\\linewidth', fig.cap="Fitted-line plots illustrating when the regression assumptions are met (upper-left) and three common assumption violations.">>=
par(mar=c(2,3,3,2),mgp=c(0.5,0,0),mfrow=c(2,2),xaxt="n",yaxt="n")
set.seed(101)
n <- 100
mu <- 0
sigma <- 10
slp <- 1
int <- 0
x <- rnorm(n,mu,sigma)
e1 <- rnorm(n,mu,sigma)
y1 <- slp*x+int+e1
lm1 <- lm(y1~x)
fitPlot(lm1,xlab="Explanatory",ylab="Response",pch=21,bg="gray70")
mtext("Assumptions Met",line=0.25,col="blue")

e2 <- ((x-mean(x))/5)^2 + rnorm(n,0,sigma/4)
y2 <- slp*x+int+e2
lm2 <- lm(y2~x)
fitPlot(lm2,xlab="Explanatory",ylab="Response",pch=21,bg="gray70")
mtext("Non-Linear",line=0.25,col="red")

x1 <- runif(n,min=0,max=10)
e3 <- rep(0,n)
for (i in 1:n) e3[i] <- rnorm(1,0,x1[i]/2)
y3 <- slp*x1+int+e3
lm3 <- lm(y3~x1)
fitPlot(lm3,xlab="Explanatory",ylab="Response",pch=21,bg="gray70")
mtext("Heteroscedastic",line=0.25,col="red")

y4 <- 2*log(x1) + rnorm(n,0,0.4)
y4 <- exp(y4)
lm4 <- lm(y4~x1)
fitPlot(lm4,xlab="Explanatory",ylab="Response",pch=21,bg="gray70")
mtext("Non-linear & Heteroscedastic",line=0.25,col="red")
@
\vspace{12pt}  %fixes spaces gobbled up by R code

In this text, if an assumption has been violated, then one should not continue to interpret the linear regression. However, in many instances, an assumption violation can be ``corrected'' by transforming one or both variables to a different scale. Transformations are not discussed in this book.

\warn{If the regression assumptions are not met, then the regression results should not be interpreted.}


\newpage
\section{Coefficient of Determination}
The coefficient of determination ($r^{2}$) is the proportion of the total variability in the response variable that is explained away by knowing the value of the explanatory variable and the best-fit model. In simple linear regression, $r^{2}$ is literally the square of $r$, the correlation coefficient.\footnote{Simple linear regression is the fitting of a model with a single explanatory variable and is the only model considered in this module and this course. See \sectref{sect:corr} for a review of the correlation coefficient.} Values of $r^{2}$ are between 0 and 1.\footnote{It is common for $r^{2}$ to be presented as a percentage.}

The meaning of $r^{2}$ can be examined by making predictions of the response variable with and without knowing the value of the explanatory variable. First, consider predicting the value of the response variable without any information about the explanatory variable. In this case, the best prediction is the sample mean of the response variable (represented by the dashed blue horizontal line in \figref{fig:CoeffDeterm}). However, because of natural variability, not all individuals will have this value. Thus, the prediction might be ``bracketed'' by predicting that the individual will be between the observed minimum and maximum values (solid blue horizontal lines). Loosely speaking, this range is the ``total variability in the response variable'' (blue box).

<<CoeffDeterm, echo=FALSE, fig.width=6, fig.height=3.5, out.width='.8\\linewidth', fig.cap="Fitted line plot with visual representations of variabilities explained and unexplained. A full explanation is in the text.">>=
## Makes some data
set.seed(199)
covmat <- matrix(c(1,0.9,0.9,1),ncol=2)
x <- rmvnorm(n=30,mean=c(0,0),sigma=covmat)
y <- 3*x[,2]+80
x <- 3*x[,1]+50
miny <- which.min(y)
maxy <- which.max(y)
avgy <- mean(y)
lm1 <- lm(y~x)
predx <- 52
predy <- predict(lm1,data.frame(x=predx))

## Makes Scatterplot
layout(matrix(1:2,nrow=1),widths=c(3,1))
par(mar=c(3,3,0.5,0),mgp=c(1.9,0.5,0),tcl=-0.2,xaxs="i",yaxs="i")
# base plot
plot(y~x,pch=21,bg="gray70",bty="l",ylim=c(74.5,85),xlim=c(44.5,55))
abline(lm1,col="gray30",lwd=2)
# total variability lines (and line at mean)
lines(c(x[miny],60),c(y[miny],y[miny]),col="darkblue",lwd=2)
lines(c(x[maxy],60),c(y[maxy],y[maxy]),col="darkblue",lwd=2)
lines(c(44.5,54),c(avgy,avgy),col="darkblue",lwd=2,lty=2)
# variability remaining lines (and line at prediction)
lines(c(predx,predx),c(predy,74.5),col="darkred",lwd=2,lty=2)
lines(c(predx,44.5),c(predy,predy),col="darkred",lwd=2,lty=2)
predymin <- predy-1.9
predymax <- predy+1.9
lines(c(predx,60),c(predymin,predymin),col="darkred",lwd=2)
lines(c(predx,60),c(predymax,predymax),col="darkred",lwd=2)


## Makes variability bars
par(mar=c(3,0,0.5,0))
plot(1,xlim=c(0,3),ylim=c(74.5,85),col="white",bty="n",xlab="",xaxt="n",yaxt="n")
# variability explained
rect(0.05,y[miny],0.95,predymin,col="darkgreen",border="darkgreen")
rect(0.05,predymax,0.95,y[maxy],col="darkgreen",border="darkgreen")
text(0.5,74.5+(predymin-74.5)/2,"Variability\nExplained",col="white",cex=1.05,font=2,srt=90)
# variability remaining lines
arrows(0,predymin,1.05,predymin,lwd=2,col="darkred",length=0.1)
arrows(0,predymax,1.05,predymax,lwd=2,col="darkred",length=0.1)
rect(1.05,predymin,1.95,predymax,col="darkred",border="darkred")
text(1.5,predy,"Variability\nRemains",col="white",cex=1.05,font=2,srt=90)
# total variability lines
arrows(0,y[miny],2.05,y[miny],lwd=2,col="darkblue",length=0.1)
arrows(0,y[maxy],2.05,y[maxy],lwd=2,col="darkblue",length=0.1)
rect(2.05,y[miny],2.95,y[maxy],col="darkblue",border="darkblue")
text(2.5,avgy,"Total Variability in Y",col="white",cex=1.1,font=2,srt=90)
@

Suppose now that the response variable is predicted for an individual with a known value of the explanatory variable (e.g., at the dashed vertical red line in \figref{fig:CoeffDeterm}). The predicted value for this individual is the value of the response variable at the corresponding point on the best-fit line (dashed horizontal red line). Again, because of natural variability, not all individuals with this value of the explanatory variable will have this exact value of the response variable. However, the prediction is now ``bracketed'' by the minimum and maximum value of the response variable \textbf{ONLY} for those individuals with the same value of the explanatory variable (solid red horizontal lines). Loosely speaking, this range is the ``variability in the response variable remaining after knowing the value of the explanatory variable'' (red box). This is the variability in the response variable that remains even after knowing the value of the explanatory variable or the variability in the response variable that cannot be explained away (by the explanatory variable).

The portion of the total variability in the response variable that was explained away consists of all the values of the response variable that would no longer be entertained as possible predictions once the value of the explanatory variable is known (green box in \figref{fig:CoeffDeterm}).

Now, by the definition of $r^{2}$, $r^{2}$ can be visualized as the area of the green box divided by the area of the blue box. This calculation does not depend on which value of the explanatory variable is chosen as long as the data are evenly distributed around the line (i.e., homoscedasticity -- see \sectref{sect:RegAssumptions}).

If the variability explained away (green box) approaches the total variability in the response variable (blue box), then $r^{2}$ approaches 1. This will happen only if the variability around the line approaches zero. In contrast, the variability explained (green box) will approach zero if the slope is zero (i.e., no relationship between the response and explanatory variables). Thus, values of $r^{2}$ also indicate the strength of the relationship; values near 1 are stronger than values near 0. Values near 1 also mean that predictions will be fairly accurate -- i.e., there is little variability remaining after knowing the explanatory variable.

\warn{A value of $r^{2}$ near 1 represents a strong relationship between the response and explanatory variables that will lead to accurate predictions.}


\section{Examples I}
There are twelve questions that are commonly asked about linear regression results. These twelve questions are listed below with some hints about things to remember when answering some of the questions. An example of these questions in context is then provided.

\begin{enumerate}
  \item What is the response variable?  \textit{Identify which variable is to be predicted or explained, which variable is dependent on another variable, which would be hardest to measure, or which is on the y-axis.}
  \item What is the explanatory variable?  \textit{The remaining variable after identifying the response variable.}
  \item Comment on linearity and homoscedasticity. \textit{Examine fitted-line plot for curvature (i.e., non-linearity) or a funnel-shape (i.e., heteroscedasticity).}
  \item What is the equation of the best-fit line?  \textit{In the generic equation of the line ($y=mx+b$) replace $y$ with the name of the response variable, $x$ with the name of the explanatory variable, $m$ with the value of the slope, and $b$ with the value of the intercept.}
  \item Interpret the value of the slope. \textit{Comment on how the response variable changes by slope amount for each one unit change of the explanatory variable, on average.}
  \item Interpret the value of the intercept. \textit{Comment on how the response variable equals the intercept, on average, if the explanatory variable is zero.}
  \item Make a prediction given a value of the explanatory variable. \textit{Plug the given value of the explanatory variable into the equation of the best-fit line. Make sure that this is not an extrapolation.}
  \item Compute a residual given values of both the explanatory and response variables. \textit{Make a prediction (see previous question) and then subtract this value from the observed value of the response. Make sure that the prediction is not an extrapolation.}
  \item Identify an extrapolation in the context of a prediction problem. \textit{Examine the x-axis scale on the fitted-line plot and do not make predictions outside of the plotted range.}
  \item What is the proportion of variability in the response variable explained by knowing the value of the explanatory variable?  \textit{This is $r^{2}$.}
  \item What is the correlation coefficient?  \textit{This is the square root of $r^{2}$. Make sure to put a negative sign on the result if the slope is negative.}
  \item How much does the response variable change if the explanatory variable changes by X units?  \textit{This is an alternative to asking for an interpretation of the slope. If the explanatory variable changes by X units, then the response variable will change by X*slope units, on average.}
\end{enumerate}

All answers should refer to the variables of the problem -- thus, ``y'', ``x'', ``response'', or ``explanatory'' should not be in any part of any answer. The questions about the slope, intercept, and predictions need to explicitly identify that the answer is an ``average'' or ``on average.''

\newpage
\subsection*{Chimp Hunting Parties}
\begin{quote}
\textit{Stanford (1996) gathered data to determine if the size of the hunting party (number of individuals hunting together) affected the hunting success of the party (number of hunts that resulted in a kill) for wild chimpanzees (Pan troglodytes) at Gombe. The results of their analysis for 17 hunting parties is shown in the figure below.\footnote{These data are in \href{https://raw.githubusercontent.com/droglenc/NCData/master/Chimp.csv}{Chimp.csv}.}  Use these results to answer the questions below.}
\end{quote}

<<ChimpFLP, echo=FALSE>>=
chimp <- read.csv("https://raw.githubusercontent.com/droglenc/NCData/master/Chimp.csv")
lm.chimp <- lm(percsucc~huntparty,data=chimp)
fitPlot(lm.chimp,pch=21,bg="gray70",
        xlab="Number of Hunting Party Members",ylab="% Successful Hunts")
r2 <- formatC(rSquared(lm.chimp),format="f",digits=3)
mtext(bquote(paste(Y==.(formatC(coef(lm.chimp)[1],format="f",digits=3))+.(formatC(coef(lm.chimp)[2],format="f",digits=3))*X,"    ",r^2==.(r2))),line=0,cex=0.8)
@

\begin{QAlist}
  \item What is the response variable?
  \begin{QAlist}
    \item The response variable is the percent of successful hunts because the authors are attempting to see if success depends on hunting party size. Additionally, the percent of successful hunts is shown on the y-axis.
  \end{QAlist}
  \item What is the explanatory variable?
  \begin{QAlist}
    \item The explanatory variable is the size of the hunting party.
  \end{QAlist}
  \item In terms of the variables of the problem, what is the equation of the best-fit line?
  \begin{QAlist}
    \item The equation of the best-fit line is \% Success of Hunt = \Sexpr{formatC(coef(lm.chimp)[1],format="f",digits=3)} + \Sexpr{formatC(coef(lm.chimp)[2],format="f",digits=3)}*Number of Hunting Party Members.
  \end{QAlist}
  \item Interpret the value of the slope in terms of the variables of the problem.
  \begin{QAlist}
    \item The slope indicates that the percent of successful hunts increases by \Sexpr{formatC(coef(lm.chimp)[2],format="f",digits=3)}, on average, for every increase of one member to the hunting party.
  \end{QAlist}
  \item Interpret the value of the intercept in terms of the variables of the problem.
  \begin{QAlist}
    \item The intercept indicates that the percent of successful hunts is \Sexpr{formatC(coef(lm.chimp)[1],format="f",digits=3)}, on average, for hunting parties with no members.
  \end{QAlist}
  \item What is the predicted hunt success if the hunting party consists of 20 chimpanzees?
  \begin{QAlist}
    \item The predicted hunt success for parties with 20 individuals is an extrapolation, because 20 is outside the range of number of members observed on the x-axis of the fitted-line plot.
  \end{QAlist}
  \item What is the predicted hunt success if the hunting party consists of 12 chimpanzees?
  \begin{QAlist}
    \item The predicted hunt success for parties with 12 individuals is \Sexpr{formatC(coef(lm.chimp)[1],format="f",digits=3)} + \Sexpr{formatC(coef(lm.chimp)[2],format="f",digits=3)}*12 = \Sexpr{formatC(coef(lm.chimp)[1]+coef(lm.chimp)[2]*12,format="f",digits=1)}\%.
  \end{QAlist}
  \item What is the residual if the hunt success for 10 individuals is 50\%?
  \begin{QAlist}
    \item The residual in this case is $50$-(\Sexpr{formatC(coef(lm.chimp)[1],format="f",digits=3)} + \Sexpr{formatC(coef(lm.chimp)[2],format="f",digits=3)}*10) = $50$-\Sexpr{formatC(coef(lm.chimp)[1]+coef(lm.chimp)[2]*10,format="f",digits=1)} = \Sexpr{formatC(50-(coef(lm.chimp)[1]+coef(lm.chimp)[2]*10),format="f",digits=1)}. Therefore, it appears that the success of this hunting party is \Sexpr{formatC(-1*(50-(coef(lm.chimp)[1]+coef(lm.chimp)[2]*10)),format="f",digits=1)}\% lower than average for this size of hunting party.
  \end{QAlist}
  \item What proportion of the variability in hunting success is explained by knowing the size of the hunting party?
  \begin{QAlist}
    \item The proportion of the variability in hunting success that is explained by knowing the size of the hunting party is $r^{2}$=\Sexpr{formatC(rSquared(lm.chimp),format="f",digits=2)}.
  \end{QAlist}
  \item What is the correlation between hunting success and size of hunting party?
  \begin{QAlist}
    \item The correlation between hunting success and size of hunting party is $r=$\Sexpr{formatC(sqrt(rSquared(lm.chimp)),format="f",digits=2)}.
  \end{QAlist}
  \item How much does hunt success decrease, on average, if there are two fewer individuals in the party?
  \begin{QAlist}
    \item If the hunting party has two fewer members, then the hunting success would decrease by \Sexpr{formatC(2*coef(lm.chimp)[2],format="f",digits=1)}\% (i.e., $-2$*\Sexpr{formatC(coef(lm.chimp)[2],format="f",digits=3)}), on average.
  \end{QAlist}
  \item Does any aspect of this regression concern you (i.e., consider the regression assumptions)?
  \begin{QAlist}
    \item The data appear to be very slightly curved but there is no evidence of a funnel-shape. Thus, the data may be slightly non-linear but they appear homoscedastic.
  \end{QAlist}
\end{QAlist}

\warn{All interpretations should be ``in terms of the variables of the problem'' rather than the generic terms of x, y, response variable, and explanatory variable.}


\section{Regression in R}
The mercury intake and amount in the blood data is loaded below to be used as an example for finding a regression line with R.
<<eval=FALSE>>=
setwd('c:/data/')
merc <- read.csv("Mercury.csv")
@
\vspace{-14pt}
<<>>=
str(merc)
@

The linear regression model is fit to two quantitative variables with \R{lm()}. The first argument is a formula of the form \R{response\TILDE explanatory}, where \R{response} contains the response variable and \R{explanatory} contains the explanatory variable, and the corresponding data.frame is in \R{data=}. The results of \R{lm()} should be assigned to an object so that specific results can be extracted.

\warn{The same formula used to make a scatterplot with \R{plot()} is used in \R{lm()} to find the best-fit line.}
<<echo=FALSE>>=
lm1 <- lm(blood~intake,data=merc)
@

The regression was fit to the mercury data below. From this it is seen that the intercept is \Sexpr{formatC(coef(lm1)[1],format="f",digits=3)} and the slope is \Sexpr{formatC(coef(lm1)[2],format="f",digits=3)}.
<<>>=
( lm1 <- lm(blood~intake,data=merc) )
@

A fitted-line plot \figrefp{fig:HGFLP} is constructed by submitting the \R{lm()} object to \R{fitPlot()}. Aspects of this plot can be adjusted using the same arguments as described for \R{plot()} in \sectref{sect:ScatterplotsR}.
<<HGFLP, fig.cap="Fitted-line plots for the regression of mercury in the blood on mercury intake.">>=
fitPlot(lm1,pch=21,bg="gray70",xlab="Mercury Intake",ylab="Mercury in the Blood")
@

Predicted values from the linear regression are obtained with \R{predict()}. The \R{predict()} function requires the saved \R{lm()} object as its first argument. The second argument is a data.frame constructed with \R{data.frame()} that contains the \textbf{EXACT} name of the explanatory variable as it appeared in \R{lm()} set equal to the value of the explanatory at which the prediction should be made. For example, the predicted amount of mercury in the blood for an intake of 240 $\mu$g per day is \Sexpr{formatC(predict(lm1,data.frame(intake=240)),format="f",digits=1)}, as obtained below.
<<>>=
predict(lm1,data.frame(intake=240))
@

The coefficient of determination is computed by submitting the saved \R{lm()} object to \R{rSquared()}. For example, \Sexpr{formatC(rSquared(lm1,percent=TRUE),format="f",digits=1)}\% of the variability in mercury in the blood is explained by knowing the amount of mercury at intake. [Note the use of \R{digits=} to control the number of decimals.]
<<>>=
rSquared(lm1,digits=3)
@


\section{Examples II}
\subsection*{Car Weight and MPG}
In \modref{chap:BivEDAQuant}, an EDA for the relationship between \var{HMPG} (the highway miles per gallon) and \var{Weight} (lbs) of 93 cars from the 1993 model year was performed. This relationship will be explored further here as an example of a complete regression analysis. In this analysis, the regression output will be examined within the context of answering the twelve typical questions. These data are read into R below and the linear regression model is fit, coefficients extracted, fitted-line plot constructed, and coefficient of determination extracted.
<<CarFit, fig.cap="Fitted line plot of the regression of highway MPG on weight of 93 cars from 1993.">>=
cars93 <- read.csv("data/93cars.csv")
( lm2 <- lm(HMPG~Weight,data=cars93) )
fitPlot(lm2,ylab="Highway MPG")
rSquared(lm2,digits=3)
@

The simple linear regression model appears to fit the data moderately well as the fitted-line plot \figrefp{fig:CarFit} shows only a very slight curvature and only very slight heteroscedasticity.\footnote{In advanced statistics books, objective measures for determining whether there is significant curvature or heteroscedasticity in the data are used. In this book, we will only be concerned with whether there is strong evidence of curvature or heteroscedasticity. There does not seem to be either here.}  The sample slope is \Sexpr{formatC(coef(lm2)[2],format="f",digits=4)}, the sample intercept is \Sexpr{formatC(coef(lm2)[1],format="f",digits=1)}, and the coefficient of determination is \Sexpr{formatC(rSquared(lm2),format="f",digits=3)}.

\begin{QAlist}
  \item What is the response variable?
  \begin{QAlist}
    \item The response variable in this analysis is the highway MPG, because that is the variable that we are trying to learn about or explain the variability of.
  \end{QAlist}
  \item What is the explanatory variable?
  \begin{QAlist}
    \item The explanatory variable in this analysis is the weight of the car (by process of elimination).
  \end{QAlist}
  \item In terms of the variables of the problem, what is the equation of the best-fit line?
  \begin{QAlist}
    \item The equation of the best-fit line for this problem is HMPG = \Sexpr{formatC(coef(lm2)[1],format="f",digits=1)} - \Sexpr{formatC(-1*coef(lm2)[2],format="f",digits=4)}Weight.
  \end{QAlist}
  \item Interpret the value of the slope in terms of the variables of the problem.
  \begin{QAlist}
    \item The slope indicates that for every increase of one pound of car weight the highway MPG decreases by \Sexpr{formatC(coef(lm2)[2],format="f",digits=4)}, on average.
  \end{QAlist}
  \item Interpret the value of the intercept in terms of the variables of the problem.
  \begin{QAlist}
    \item The intercept indicates that a car with 0 weight will have a highway MPG value of \Sexpr{formatC(coef(lm2)[1],format="f",digits=1)}, on average.\footnote{This is the correct interpretation of the intercept. However, it is nonsensical because it is an extrapolation; i.e., no car will weigh 0 pounds.}
  \end{QAlist}
  \item What is the predicted highway MPG for a car that weighs 3100 lbs?
  \begin{QAlist}
    \item The predicted highway MPG for a car that weighs 3100 lbs is \Sexpr{formatC(coef(lm2)[1],format="f",digits=5)} - \Sexpr{formatC(-1*coef(lm2)[2],format="f",digits=5)}(3100) = \Sexpr{formatC(predict(lm2,data.frame(Weight=3100)),format="f",digits=1)} MPG. Alternatively, this value is computed with
<<>>=
predict(lm2,data.frame(Weight=3100))
@
  \end{QAlist}
  \item What is the predicted highway MPG for a car that weighs 5100 lbs?
  \begin{QAlist}
    \item The predicted highway MPG for a car that weighs 5100 lbs should not be computed with the results of this regression, because 5100 lbs is outside the domain of the data \figrefp{fig:CarFit}.
  \end{QAlist}
  \item What is the residual for a car that weights 3500 lbs and has a highway MPG of 24?
  \begin{QAlist}
    \item The predicted highway MPG for a car that weighs 3500 lbs is \Sexpr{formatC(coef(lm2)[1],format="f",digits=5)} - \Sexpr{formatC(-1*coef(lm2)[2],format="f",digits=5)}(3500) = \Sexpr{formatC(predict(lm2,data.frame(Weight=3500)),format="f",digits=1)}. Thus, the residual for this car is 24 - \Sexpr{formatC(predict(lm2,data.frame(Weight=3500)),format="f",digits=1)} = \Sexpr{formatC(24-predict(lm2,data.frame(Weight=3500)),format="f",digits=1)}. Alternatively, this is computed in R with
<<>>=
24-predict(lm2,data.frame(Weight=3500))
@
Therefore, it appears that this car gets \Sexpr{formatC(-1*(24-predict(lm2,data.frame(Weight=3500))),format="f",digits=1)} MPG less than an average car with the same weight.
  \end{QAlist}
  \item What proportion of the variability in highway MPG is explained by knowing the weight of the car?
  \begin{QAlist}
    \item The proportion of the variability in highway MPG that is explained by knowing the weight of the car is $r^{2}$=\Sexpr{formatC(rSquared(lm2),format="f",digits=2)}.
  \end{QAlist}
  \item What is the correlation between highway MPG and car weight?
  \begin{QAlist}
    \item The correlation between highway MPG and car weight is $r=$\Sexpr{formatC(-1*sqrt(rSquared(lm2)),format="f",digits=2)}.\footnote{Put a negative sign in front of your result from taking the square root of $r^2$, because the relationship between highway MPG and weight is negative.}
  \end{QAlist}
  \item How much is the highway MPG expected to change if a car is 1000 lbs heavier?
  \begin{QAlist}
    \item If the car was 1000 lbs heavier, you would expect the car's highway MPG to decrease by \Sexpr{formatC(-1000*coef(lm2)[2],format="f",digits=2)} (i.e., 1000 slopes).
  \end{QAlist}
\end{QAlist}





\newpage
\begin{exsection}
  \item \label{revex:RegRivers1} \cite{Dudgeon2000}, while describing the features of major tropical Asian rivers, examined the relationship between the length (km) and drainage area (km$^{2}$) of 11 waterways. In particular he wanted to determine if a model could be produced that would allow the drainage area of the river to be predicted from the length of the river. Identify the response and explanatory variables. Explain your choices. \ansref{ans:RegRivers1}
  \item \label{revex:RegBirth1} Researchers collected data on 56 normal births at a Wellington, New Zealand hospital. They were interested in determining if the weight of the newborn child (labeled as \var{BirthWt}) could be predicted by knowing the mothers age (labeled as \var{Age}). Identify the response and explanatory variables. Explain your choices. \ansref{ans:RegBirth1}
\end{exsection}


\begin{exsection}
  \item \label{revex:RegRivers2} The research described in Review Exercise \ref{revex:RegRivers1} identified the best-fit line equation as $Area=-159131+314.229Length$. \ansref{ans:RegRivers2}
    \begin{Enumerate}
      \item What is the response variable?
      \item Interpret the value of the slope in terms of the variables of this problem.
      \item Interpret the value of the intercept in terms of the variables of this problem.
      \item If one river was 10 km longer than another river, then how much more area would you expect it to drain?
    \end{Enumerate}

  \item \label{revex:RegBirth2} The research described in Review Exercise \ref{revex:RegBirth1} computed the following regression results: $BirthWt=2054+51.7Age$. \ansref{ans:RegBirth2}
    \begin{Enumerate}
      \item What is the explanatory variable?
      \item Interpret the value of the slope in terms of the variables of this problem.
      \item Interpret the value of the intercept in terms of the variables of this problem.
      \item Assume that a mother had a child when she was 20 and when she was 25. On average, how much more or less would you expect, based on these findings, the second child to weigh compared to the first child?
    \end{Enumerate}
\end{exsection}

\begin{exsection}
  \item \label{revex:RegRivers3} Use the results described in Review Exercise \ref{revex:RegRivers2} to answer the questions below. \ansref{ans:RegRivers3}
    \begin{Enumerate}
      \item Predict the drainage area for a river 3500 km long.
      \item Calculate the residual if the river above (3500 km length) had a drainage area of 1,000,150 km$^{2}$.
      \item Predict the drainage area for a river 7500 km long.
    \end{Enumerate}

  \item \label{revex:RegBirth3} Use the results described in Review Exercise \ref{revex:RegBirth2} to answer the questions below. \ansref{ans:RegBirth3}
    \begin{Enumerate}
      \item Predict the weight of a child born to a 30-year-old mother.
      \item A 30-year-old mother had a child that weighed 3550 g. Find the residual for that mother.
      \item Predict the weight of a child born to an 18-year-old mother.
    \end{Enumerate}

  \item \label{revex:RegLumina} Researchers at Chevrolet attempted to determine the relationship between gas mileage (MPG) of Luminas in the city (CITY) and on the highway (HIGHWAY). Results of their analysis is shown below. \ansref{ans:RegLumina}
<<LuminaFLP, echo=FALSE, results='hide', include=FALSE, fig.width=3.5, fig.height=3.5, par1a=TRUE>>=
lum <- read.csv("data/Lumina.csv")
lm.lum <- lm(CMPG~HMPG,data=lum)
fitPlot(lm.lum,pch=16,xlab="Highway MPG",ylab="City MPG")
r2 <- rSquared(lm.lum,digits=3)
mtext(bquote(paste(Y==.(round(coef(lm.lum)[1],3))+.(round(coef(lm.lum)[2],3))*X,"    ",r^2==.(r2))),line=0.2,cex=0.8)
@
\begin{center}
  \includegraphics[width=3in]{Figs/LuminaFLP-1}
\end{center}
    \begin{Enumerate}
      \item Predict the city mpg for a Lumina that gets 25 mpg on the highway.
      \item Predict the highway mpg for a Lumina that gets 25 mpg in the city.
      \item Predict the city mpg for a Lumina that gets 40 mpg on the highway.
      \item What is the residual for a Lumina that gets 25 mpg on the highway and 20 in the city?
    \end{Enumerate}
\end{exsection}


\begin{exsection}
  \item \label{revex:RegFlyC} \rhw{} \cite{WangFinch1997} hypothesized that larger willow flycatchers (\emph{Empidonax traillii}) migrated up the Middle Rio Grande River earlier than small willow flycatchers. To test this hypothesis they captured flycatchers on several days during their migration and measured the wing length (mm; an index of overall body size) of each bird. They recorded the date that the bird was captured as a Julian date (days since Jan. 1).  The results of their study are found in \href{https://raw.githubusercontent.com/droglenc/NCData/master/Flycatcher.csv}{Flycatcher.csv}. Load these data into R and produce results that can be used to answer the questions below. \ansref{ans:RegFlyC}
    \begin{Enumerate}
      \item What is the explanatory variable?
      \item What is the response variable?
      \item In terms of the variables of this problem, what is the equation of the best-fit line?
      \item In terms of the variables of this problem, interpret the value of the intercept.
      \item In terms of the variables of this problem, interpret the value of the slope.
      \item How much different do you expect the wing length to be ten days later?
      \item What is the predicted wing length on day 180?
      \item What is the residual for a bird with wing length 66.5 on day 151?
      \item What proportion of the variability in wing length is explained by knowing the date?
      \item What is the correlation coefficient between wing length and date?
      \item Comment on the assumptions of the linear regression.
    \end{Enumerate}

  \item \label{revex:RegFat} \rhw{} \cite{Carroll1975} examined the relationship between per capita consumption of animal fat (g/day; AnimFatI) and age-adjusted death rate from breast cancer (AgeAdjDe) for 39 countries. Her goal was to determine if variability in the breast cancer death rate could be explained by the amount of fat consumed. The data for their study are found in \href{https://raw.githubusercontent.com/droglenc/NCData/master/CancerFat.csv}{CancerFat.csv}. Load these data into R and produce results that can be used to answer the questions below. \ansref{ans:RegFat}
    \begin{Enumerate}
      \item Which variable is the response variable?
      \item What is an individual in this study?
      \item In terms of the variables of this problem, what is the equation of the best-fit line?
      \item In terms of the variables of this problems, interpret the value of the slope.
      \item If country A consumes 4 g/day less animal fat than country B, how much different will the predicted age adjusted death rate due to breast cancer be for country A?
      \item What is the predicted age adjusted death rate due to breast cancer for a country that consumes 170 g/day of animal fat?
      \item What is the residual for a country that consumes 90 g/d of animal fat and has an age adjusted death rate due to breast cancer of 14.5?
      \item What is the correlation coefficient between the age adjusted death rate and the intake of animal fat?
      \item How much of the variability in a country's age adjusted death rate due to breast cancer is explained by knowing the value of its animal fat intake?
      \item Can it be said that an increase in intake of animal fat is the cause for an increase in the age adjusted death rate due to breast cancer? Why or why not?
    \end{Enumerate}

  \item \label{revex:RegRIFA} \rhw{} \cite{Allenetal1997} investigated the impact of the density of red-imported fire ants (\emph{Solenopsis invicta}; RIFA) on the recruitment of white-tailed deer (\emph{Odocoileus virginianus}) fawns (an index of does to fawns). A modified version of their results are found in \href{https://raw.githubusercontent.com/droglenc/NCData/master/RIFA.csv}{RIFA.csv}. Load these data into R and produce results that can be used to answer the questions below. \ansref{ans:RegRIFA}
    \begin{Enumerate}
      \item What is the response variable?
      \item What is the explanatory variable?
      \item In terms of the variables of this problem, what is the equation of the best-fit line?
      \item In terms of the variables of this problem, interpret the value of the slope.
      \item If the RIFA index increases by 500, how much different do you expect fawn recruitment to be?
      \item What is the predicted fawn recruitment when the RIFA index is 1700?
      \item What is the residual when the RIFA index is 2700 and fawn recruitment is 0.3?
      \item What is the correlation coefficient between RIFA and fawn recruitment?
      \item What proportion of the variability in fawn recruitment is explained by knowing the RIFA index?
      \item Comment on the assumptions in this regression.
    \end{Enumerate}

  \item \label{revex:RegMathAss} \rhw{} All incoming freshmen are required to take a math assessment test to determine which math classes they should take. Sometimes pre-registering students will register before taking the assessment. To make the best possible course choices for these students, the adviser would like to predict their assessment score (ASSESS) based on their math ACT scores (ACT). The ACT score and assessment score from 72 freshmen from 2003 are stored in \href{https://raw.githubusercontent.com/droglenc/NCData/master/NCAssess.csv}{NCAssess.csv}. Load these data into R and produce results that can be used to answer the questions below. \ansref{ans:RegMathAss}
    \begin{Enumerate}
      \item What is the explanatory variable?
      \item In terms of ACT and Assessment test scores, what does the value of the slope mean?
      \item Mary Lamb had an ACT score of 40. Predict her assessment score.
      \item John Tukey had an ACT score of 19. Predict his assessment score.
      \item John Tukey actually scored a 15 on his assessment test. Calculate his residual?
      \item What proportion of the variability in assessment score is explained by knowing the ACT score?
      \item What are the two most important assumptions in a regression analysis. Are these violated for this data set?  Why or why not?
      \item Do you think that these results provide a useful predictor of math assessment scores in cases where those scores are not available but ACT scores are? Explain.
    \end{Enumerate}

  \item \label{revex:RegDNA} \rhw{} \cite{SuitBauer1990} examined DNA indices obtained from fresh and frozen tissue samples with the goal of determining if fresh values could be predicted from frozen values. The data for their study are found in \href{https://raw.githubusercontent.com/droglenc/NCData/master/DNA.csv}{DNA.csv}. Load these data into R and produce results that can be used to answer the questions below. Note that one outlier should be excluded from the analysis. \ansref{ans:RegDNA}
    \begin{Enumerate}
      \item What did the researchers consider as the response variable?
      \item What is the equation of the best-fit line in terms of the variables of the problem?
      \item Interpret the value of the slope in terms of the variables of the problem.
      \item What is the predicted fresh index if the frozen index is 4.05?
      \item What is the residual for a fresh index of 2.1 and a frozen index of 2.2?
      \item What proportion of the variability in the fresh index is explained by knowing the frozen index?
      \item What is the correlation between the fresh and frozen indices?
      \item What are the two major assumptions of regression and do they look like they've been met with these data (be specific!)?
    \end{Enumerate}

  \item \label{revex:RegFawns} \rhw{} Wildlife ecologist in Texas wanted to determine if the amount of precipitation could explain some of the variability observed in the number of fawns born to each doe \citep{GinnettYoung2000}. Because Texas has many different climatic regions, the state was broken down into eight precipitation zones, and the mean precipitation for each zone over a period of five years was calculated. Furthermore, the researchers measured the mean number of fawns born per 100 does for each of these five years. The data for their study are found in \href{https://raw.githubusercontent.com/droglenc/NCData/master/Deer1.csv}{deer1.csv}. Load these data into R and produce results that can be used to answer the questions below. \ansref{ans:RegFawns}
    \begin{Enumerate}
      \item Express the equation of the best-fit line in terms of the variables of the problem.
      \item Interpret the slope of the best-fit line in terms of the variables.
      \item If the mean precipitation in an area were 1500 mm, how many fawns per 100 does would you expect?
      \item If a precipitation zone has a mean precipitation of 1050 mm and an average of 37 fawns per 100 does, what is the residual of this zone?
      \item What is the correlation coefficient between mean no. of fawns per 100 does and mean precipitation?
      \item What proportion of the variability in the mean number of fawns per 100 does is explained by knowing the mean precipitation?
      \item If the average amount of precipitation increases by 100 mm, how many more fawns per 100 does would you expect to be born?
    \end{Enumerate}

  \item \label{revex:RegChirps} \rhw{} It has been said that temperature can be estimated from the number of cricket chirps heard. To determine if this relationship existed, an entomologist recorded the number of chirps in a 15-second interval by crickets held at different temperatures. The data for their study are found in \href{https://raw.githubusercontent.com/droglenc/NCData/master/Chirps.csv}{Chirps.csv}. Load these data into R and produce results that can be used to answer the questions below. \ansref{ans:RegChirps}
    \begin{Enumerate}
      \item What is the response variable?
      \item What is the explanatory variable?
      \item In terms of the variables of this problem, what is the equation of the best-fit line?
      \item In terms of the variables of this problem, interpret the value of the slope.
      \item If the number of chirps increases by 5, then how much different do you expect temperature to be?
      \item If you hear 18 chirps during the day and 15 chirps at night, then how much different is the temperature, on average?
      \item What is the residual when you hear 12 chirps and the temperature is 65 F?
      \item What is the correlation coefficient between temperature and the number of chirps?
      \item What proportion of the variability in temperature is explained by knowing the number of chirps?
      \item Construct a residual plot and use it to interpret the validity of regression assumptions.
    \end{Enumerate}

\end{exsection}
