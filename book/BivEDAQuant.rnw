<<echo=FALSE, cache=FALSE, results='hide'>>=
set_parent('IntroStats.Rnw')
@

\chapter{Bivariate EDA - Quantitative} \label{chap:BivEDAQuant}

\vspace{-30pt}
\minitoc
\vspace{18pt}

\lettrine{B}{ivariate data occurs when two} variables are measured on the same individuals. For example, you may measure (i) the height and weight of students in class, (ii) depth and area of a lake, (iii) gender and age of welfare recipients, or (iv) number of mice and biomass of legumes in fields. This module is focused on describing the bivariate relationship between two quantitative variables. Bivariate relationships between two categorical variables is described in \modref{chap:BivEDACat}.

Data on the \var{weight} (lbs) and highway miles per gallon (\var{HMPG}) for 93 cars from the 1993 model year will be used as an example throughout this module. Ultimately, the relationship between highway MPG and the weight of a car will be described. These data are read from \href{https://raw.githubusercontent.com/droglenc/NCData/master/93cars.csv}{93cars.csv} into R and several observations of \var{HMPG} and \var{weight} are shown below.\footnote{The vector in the second argument to \R{headtail()} is used to show only the two variables of interest.}
<<>>=
cars93 <- read.csv("data/93cars.csv")
headtail(cars93,which=c("HMPG","Weight"))
@



\section[Response and Explanatory] {Response and Explanatory Variables} \label{sect:RespExplan1}
\vspace{-3pt}
The \textbf{response variable} is the variable that one is interested in explaining something (i.e., variability) or in making future predictions about. The \textbf{explanatory variable} is the variable that may help explain or allow one to predict the response variable. In general, the response variable is thought to depend on the explanatory variable. Thus, the response variable is often called the \textbf{dependent variable}, whereas the explanatory variable is often called the \textbf{independent variable}.

One may identify the response variable by determining which of the two variables depends on the other. For example, in the car data, highway MPG is the response variable because gas mileage is most likely affected by the weight of the car (e.g., hypothesize that heavier cars get worse gas mileage), rather than vice versa.

However, in other situations, it is not immediately clear which variable is the response. For example, does the number of mice in the field depend on the number of legumes (lots of feed=lots of mice) or the other way around (lots of mice= not much of their food left)? Similarly, does area depend on depth or does depth depend on area of the lake? In these situations, the context of the research question is needed to identify the response variable. For example, if the researcher hypothesized that number of mice will be greater if there is more biomass of legumes, then number of mice is the response variable. In many cases, the more difficult variable to measure will most likely be the response variable. For example, researchers likely wish to predict area of a lake (hard to measure) from depth of the lake (easy to measure).

\vspace{-9pt}
\warn{Which variable is the response may depend on the context of the research question.}


\vspace{-12pt}
\section{Summaries}
\vspace{-6pt}
\subsection{Scatterplots}
\vspace{-3pt}
A scatterplot is a graph where each point simultaneously represents the values of both the quantitative response and quantitative explanatory variable. The value of the explanatory variable gives the x-coordinate and the value of the response variable gives the y-coordinate of the point plotted for an individual. For example, the first individual in the cars data is plotted at x (\var{Weight}) = \Sexpr{cars93$Weight[1]} and y (\var{HMPG}) = \Sexpr{cars93$HMPG[1]}, whereas the second individual is at x = \Sexpr{cars93$Weight[2]} and y = \Sexpr{cars93$HMPG[2]} \figrefp{fig:carscat2}.
<<carscat1, echo=FALSE, fig.show='hide'>>=
plot(HMPG~Weight,data=cars93,pch=16,xlab="Weight (lbs)",ylab="Highway MPG")
@

<<carscat2, echo=FALSE, fig.cap="Scatterplot between the highway MPG and weight of cars manufactured in 1993. For reference to the main text, the first individual is red and the second individual is green.">>=
<<carscat1>>
points(HMPG~Weight,data=cars93[1:2,],pch=16,col=c("red","green"))
@

Scatterplots are constructed in R with \R{plot()}. This function requires a formula of the form \R{Y\TILDE X}, where \R{Y} and \R{X} are variables to be plotted on the y- and x-axes, as the first argument, and the corresponding dataframe name in \R{data=}. The x- and y-axis labels may be modified with \R{xlab=} and \R{ylab=}. The character plotted at each point can be changed with the \R{pch=} argument.\footnote{This argument is short for ``plotting character''.} This argument defaults to a value of 1, which is an open-circle. Numerical values used to represent other plotting characters are shown in \figref{fig:Rpch}.

<<Rpch, echo=FALSE, fig.width=4, fig.height=2, fig.cap="Plotting characters available in R and their numerical codes. Note that for values of 21-25 that \\R{bg='gray70'} is used to provide the background color.">>=
par(mar=c(0,0,0,0))
plot(1,xlim=c(0.5,7.75),ylim=c(0.5,3.5),type="n",ann=FALSE,
     xaxt="n",yaxt="n",bty="n",yaxs="i",xaxs="i")
pchs <- c(0:6,15:20,NA,21:25,NA,NA)
xs <- rep(1:7,3)
ys <- rep(3:1,each=7)
points(xs,ys,pch=pchs,cex=1.5,bg="gray70")
text(xs+0.4,ys,pchs,cex=1.2)
box()
@

The scatterplot, excluding the two highlighted points, of highway MPG versus car weight \figrefp{fig:carscat2} was created with the code below.
<<eval=FALSE>>=
<<carscat1>>
@


\subsection{Correlation}\label{sect:corr}
The sample correlation coefficient, abbreviated as $r$, is calculated with
\begin{equation} \label{eqn:Correlation}
  r = \frac{\Sum_{i=1}^{n}\left[\left(\frac{x_{i}-\bar{x}}{s_{x}}\right)\left(\frac{y_{i}-\bar{y}}{s_{y}}\right)\right]}{n-1}
\end{equation}
where $s_{x}$ and $s_{y}$ are the sample standard deviations for the explanatory and response variable, respectively.\footnote{See \sectref{sect:StdDev} for a review of standard deviations.} The formulae in the two sets of parentheses in the numerator are standardized values; thus, the value in each parenthesis is called the standardized x or standardized y, respectively.\footnote{See \sectref{sect:Standardizing} for a review of standardized values.} Using this terminology, Equation \eqref{eqn:Correlation} reduces to these steps:
\begin{Enumerate}
  \item For each individual, standardize x and standardize y.
  \item For each individual, find the product of the standardized x and standardized y.
  \item Sum all of the products from step 2.
  \item Divide the sum from step 3 by n-1.
\end{Enumerate}

The table below illustrates these calculations for the first five individuals in the cars data.\footnote{The five cars are treated as if they are the entire sample.} Note that the ``i'' column is an index for each individual, the $x_{i}$ and $y_{i}$ columns are the observed values of the two variables for individual $i$, $\bar{x}$ was computed by dividing the sum of the $x_{i}$ column by $n$, $s_{x}$ was computed by dividing the sum of the $(x_{i}-\bar{x})^{2}$ column by $n-1$ and taking the square root, and the ``std x'' column is the standardized x values found by dividing the value in the $x_{i}-\bar{x}$ column by $s_{x}$. Similar calculations were made for the y variable. The final correlation coefficient is the sum of the last column divided by $n-1$. Thus, the correlation between car weight and highway mpg for these five cars is -0.54.

\begin{center}
  \begin{tabular}{cccccccccc}
\hline\hline
 & HMPG & Weight & & & & & & & \\
i & $y_{i}$ & $x_{i}$ & $y_{i}-\bar{y}$ & $x_{i}-\bar{x}$ & $(y_{i}-\bar{y})^{2}$ & $(x_{i}-\bar{x})^{2}$ & std. y & std. x & (std. y)(std. x) \\
\hline
1 & 31 & 2705 &  3.4 & -632 & 11.56 & 399424 &  1.26 & -1.71 & -2.15 \\
2 & 25 & 3560 & -2.6 &  223 &  6.76 &  49729 & -0.96 &  0.6  & -0.58 \\
3 & 26 & 3375 & -1.6 &   38 &  2.56 &   1444 & -0.59 &  0.1  & -0.06 \\
4 & 26 & 3405 & -1.6 &   68 &  2.56 &   4624 & -0.59 &  0.18 & -0.11 \\
5 & 30 & 3640 &  2.4 &  303 &  5.76 &  91809 &  0.89 &  0.82 &  0.73 \\
\hline
sum & 138 & 16685 & 0 & 0 & 29.2 & 547030 & 0 & 0 &  -2.17 \\
\hline\hline
  \end{tabular}
\end{center}

The meaning and interpretation of $r$ is discussed in more detail in \sectref{sect:BivEDAItems}.

The correlation coefficient ($r$) between two quantitative variables is computed with \R{corr()}, with a formula of the form \R{Y\TILDE X} or \R{\TILDE Y+X}, where \R{Y} and \R{X} are the names of quantitative variables, as the first argument and the corresponding data.frame in \R{data=}. For example, the correlation coefficient between highway MPG and weight for the car data is \Sexpr{formatC(corr(HMPG~Weight,data=cars93),format="f",digits=2)}.
<<>>=
corr(HMPG~Weight,data=cars93)
corr(~HMPG+Weight,data=cars93)  # alternative form
@


\subsection{Pairs of Multiple Variables}
For efficiency, a matrix of scatterplots or correlation coefficients may be constructed for many pairs of variables. Each subplot in the scatterplot matrix \figrefp{fig:Scatplot4} is a scatterplot with the variable listed in the same column on the x-axis and the variable listed in the same row on the y-axis. For example, the scatterplot in the upper-right corner of \figref{fig:Scatplot4} has highway MPG on the y-axis and car weight on the x-axis. Similarly, the correlation between two variales is in the row and column of the correlation matrix that contains the names of the variables.

A scatterplot matrix is constructed with \R{pairs()}, which takes a formula of the form \R{\TILDE X1+X2+X3} (and so on), where the \R{X1}, \R{X2}, etc. are all quantitative variables, and the corresponding data.frame in \R{data=}. Similarly, the correlation matrix is constructed by including the same two arguments in \R{corr()}.

In some instances, the data.frame may contain missing values (i.e., data that were not recorded). The individuals with missing data are efficiently removed from the correlation matrix with \R{use="pairwise.complete.obs"} in \R{corr()}.\footnote{Missing data are automatically removed from the scatterplots.} The number of digits reported in the correlation matrix is controlled with \R{digits=}.

<<echo=FALSE, results='hide'>>=
carscorr <- corr(~HMPG+FuelTank+Length+Weight,data=cars93,
                 use="pairwise.complete.obs")
@

For example, the scatterplot matrix \figrefp{fig:Scatplot4} and correlation matrix for all pairs of four variables in the cars data are computed below. Thus, the correlation between highway MPG and size of the fuel tank is \Sexpr{formatC(carscorr["HMPG","FuelTank"],format="f",digits=3)}, whereas the correlation between length and weight of the car is \Sexpr{formatC(carscorr["Length","Weight"],format="f",digits=3)}.
<<Scatplot4, fig.width=6, fig.height=6, out.width='.7\\linewidth', fig.cap="Scatterplot matrix of the highway MPG, fuel tank size, length, and weight of cars.">>=
pairs(~HMPG+FuelTank+Length+Weight,data=cars93)
corr(~HMPG+FuelTank+Length+Weight,data=cars93,use="pairwise.complete.obs",digits=3)
@


\section{Items to Describe} \label{sect:BivEDAItems}
Four characteristics should be described for a bivariate EDA with two quantitativariables:
\vspace{-8pt}
\begin{Enumerate}
  \item \textbf{form} of the relationship,
  \item presence (or absence) of \textbf{outliers}, and
  \item \textbf{association} or \textbf{direction} of the relationship,
  \item \textbf{strength} of the relationship.
\end{Enumerate}
\vspace{-4pt}
All four of these items can be described from a scatterplot. However, for certain relationships (discussed below), strength is best described from the correlation coefficient.

The form of a relationship is determined by whether the ``cloud'' of points on a scatterplot forms a line or some sort of curve \figrefp{fig:corrassn}. For the purposes of this introductory course, if the ``cloud'' appears linear then the form will said to be linear, whereas if the ``cloud'' is curved then the form will be nonlinear. Scatterplots should be considered \textbf{linear} unless there is an OBVIOUS curvature in the points.

<<forms, echo=FALSE, out.width='.3\\linewidth', fig.cap="Depictions of two linear (Left and Center) and one nonlinear (Right) relationship.">>=
par(mar=c(0.5,0.5,2,0.5),xaxt="n",yaxt="n")
set.seed(1054)
r <- c(-.9,0.4)
for (i in 1:length(r)) {
  plot(rmvnorm(n=100,mean=c(0,0),sigma=matrix(c(1,r[i],r[i],1),ncol=2)),
       xlab="",ylab="",pch=21,bg="gray70")
  mtext("Linear",cex=1.5,line=0.5)
}
x <- 3*runif(100)
y <- 2.5*(x^2)-3*x + rnorm(100,sd=1.5)
plot(y~x,xlab="",ylab="",pch=21,bg="gray70")
mtext("Nonlinear",cex=1.5,line=0.5)
@

An outlier is a point that is far removed from the main cluster of points. Keep in mind (as always) that just because a point is an outlier doesn't mean it is wrong.

A positive association is when the scatterplot resembles an increasing function (i.e., increases from lower-left to upper-right; \figref{fig:corrassn}-Left). For a positive association, most of the individuals are above average or below average for both of the variables. A negative association is when the scatterplot looks like a decreasing function (i.e., decreases from upper-left to lower-right; \figref{fig:corrassn}-Right). For a negative association, most of the individuals are above average for one variable and below average for the other variable. No association is when the scatterplot looks like a ``shotgun blast'' of points (\figref{fig:corrassn}-Center). For no association, there is no tendency for individuals to be above or below average for one variable and above or below average for the other.

<<corrassn, echo=FALSE, out.width='.3\\linewidth', fig.cap="Depiction of three types of association present in scatterplots.">>=
set.seed(1054)
r <- c(.8,0,-.7)
lbls <- c("Positive","None","Negative")
par(mar=c(0.5,0.5,2,0.5),xaxt="n",yaxt="n")
for (i in 1:length(r)) {
  plot(rmvnorm(n=100,mean=c(0,0),sigma=matrix(c(1,r[i],r[i],1),ncol=2)),
       xlab="",ylab="",pch=21,bg="gray70")
  mtext(lbls[i],cex=1.5,line=0.5)
}
@

Strength is a summary of how closely the points cluster about the general form of the relationship. For example, if a linear form exists, then strength is how closely the points cluster around the line. Strength is difficult to define from a scatterplot because it is a relative term. However, the correlation coefficient ($r$; \sectref{sect:corr}) is a measure of strength (and association) between two variables, \textit{if the form is linear}.

The sign of $r$ indicates the association between the two variables. A positive $r$ means a positive association and a negative $r$ means a negative association. The absolute value of $r$ (i.e., the value ignoring the sign) is an indicator of strength of relationship. Absolute values nearer 1 are stronger relationships.

To better understand how $r$ is a measure of association and strength, reconsider the steps in calculation $r$ from \sectref{sect:corr}. The scatterplots in \figref{fig:corrdefn1} represent a positive (Left) and negative (Right) association. These scatterplots have dashed lines at the mean of both the x- and y-axis variables. Because the mean is substracted from observed values when standardizing, points that fall above the mean will have positive standardized values and points that fall below the mean will have negative standardized values. The sign for the standardized values are depicted along the axes.

<<corrdefn1, echo=FALSE, fig.width=7, out.width='.8\\linewidth', fig.cap="Scatterplot with mean lines superimposed and the signs of standardized values for both x and y shown for a positive (\\textbf{Left}) and negative (\\textbf{Right}) association. Dark gray points have a positive product of standardized values, whereas red points have a negative product of standardized values.">>=
set.seed(16502)
r <- c(0.8,-0.8)
par(mar=c(2,2,1,1),mgp=c(2,0.5,0),mfcol=c(1,2),xaxs="i",yaxs="i")
for (i in 1:length(r)) {
  x <- rmvnorm(n=100,mean=c(0,0),sigma=matrix(c(1,r[i],r[i],1),ncol=2))
  y <- x[,2]; x <- x[,1]
  xbar <- mean(x); ybar <- mean(y)
  plot(-5,xlab="",ylab="",xlim=c(-3,3),ylim=c(-3,3),xaxt="n",yaxt="n")
  axis(1,at=xbar,c(expression(bar(x))),cex=1.5)
  axis(2,at=c(ybar),c(expression(bar(y))),cex=1.5)
  abline(v=xbar,lty=2,lwd=2,col="gray60")
  abline(h=ybar,lty=2,lwd=2,col="gray60")
  axis(1,at=c(-2,2),c("Below (-)","Above (+)"),cex=2.5,tick=FALSE)
  axis(2,at=c(-2,2),c("Below (-)","Above (+)"),cex=2.5,tick=FALSE)
  points(x[(x>xbar & y<ybar)|(x<xbar & y>ybar)],
         y[(x>xbar & y<ybar)|(x<xbar & y>ybar)],pch=21,bg="red")
  points(x[(x>xbar & y>ybar)|(x<xbar & y<ybar)],
         y[(x>xbar & y>ybar)|(x<xbar & y<ybar)],pch=21,bg="gray30")
}
@

Now consider the product of standardized x's and y's in each quadrant of the scatterplots in \figref{fig:corrdefn1}. In the quadrant where both standardized values are above average (i.e., both positive signs) the product of standardized values is positive (dark gray points). In the quadrant where both standardized values are negative, the product of standardized values is also positive. In the other two quadrants, the product of standardized values is negative (red points).

Thus, for a positive association (\figref{fig:corrdefn1}-Left) the numerator of the correlation coefficient is positive because it is the sum of many positive (dark gray points) and few negative (red points) products of standardized values. The denominator (recall that it is $n-1$) is always positive. Therefore, $r$ for a positive association is positive. Conversely, for a negative association (\figref{fig:corrdefn1}-Right) the numerator of the correlation coefficient is negative because it is the sum of few positive (dark gray points) and many negative (red points) products of standardized values. Therefore, $r$ for a negative association is negative.

Correlations range from -1 to 1. Absolute values of $r$ equal to 1 indicate a perfect correlation; i.e., all points fall exactly on a line. A correlation of 0 indicates no association. Thus, absolute values of $r$ near 1 indicate strong relationships and those near 0 are weak. The range of correlation values and a few scatterplots illustrating how the strength and association of the relationship changes along this scale is illustrated in \figref{fig:corrstrength2}. The categorizations in \tabref{tab:StrengthCriteria} can be used as a rough guideline for categorizing the strength of a relationship between two variables.

<<corrstrength2, echo=FALSE, fig.width=8, fig.height=1.5, out.width='.95\\linewidth', fig.cap="Scatterplots along the continuum of $r$ values.">>=
par(mar=c(0.2,0.2,2.8,0.2),mfcol=c(1,7))
set.seed(1909)
r <- c(-1,-0.8,-0.4,0,0.4,0.8,1)
for (i in 1:length(r)) {
  covmat <- matrix(c(1,r[i],r[i],1),ncol=2)
  x <- rmvnorm(n=100,mean=c(0,0),sigma=covmat)
  y <- x[,2]; x <- x[,1]
  xbar <- mean(x); ybar <- mean(y)
  plot(-5,xlab="",ylab="",xaxt="n",yaxt="n",xlim=c(-3,3),ylim=c(-3,3))
  abline(v=xbar,lty=2,lwd=1); abline(h=ybar,lty=2,lwd=1)
  points(x[(x>xbar & y<ybar)|(x<xbar & y>ybar)],
         y[(x>xbar & y<ybar)|(x<xbar & y>ybar)],pch=21,bg="red",cex=0.8,lwd=0.5)
  points(x[(x>xbar & y>ybar)|(x<xbar & y<ybar)],
         y[(x>xbar & y>ybar)|(x<xbar & y<ybar)],pch=21,bg="gray30",cex=0.8,lwd=0.5)
  if (r[i]==-1 | r[i]==1) {
    mtext("Strongest",line=1.6)
    mtext(paste("r=",r[i]),line=0.2)
  } else if (r[i]==0) {
    mtext("Weakest",line=1.6)
    mtext(paste("r=",r[i]),line=0.2)
  }
}
@

\begin{table}[htbp]
  \caption{Classifications of strength of relationship for absolute values of $r$ by type of study.}
  \label{tab:StrengthCriteria}
  \centering
  \begin{tabular}{c|ccc}
\hline\hline
\widen{0}{5}{Strength of} & Uncontrolled/ & Controlled/ \\
\widen{-2}{0}{Relationship} & Observational & Experimental \\
\hline
\widen{0}{4}{Strong} & $>0.8$ & $>0.95$ \\
\widen{0}{4}{Moderate} & $>0.6$ & $>0.9$ \\
\widen{-1}{5}{Weak} & $>0.4$ & $>0.8$ \\
\hline\hline
  \end{tabular}
\end{table}

\section{Synthetic Interpretations}
When performing a bivariate EDA for two quantatitative variables, the form, presence (or absence) of outliers, association, and strength should be specifically addressed. In addition, you should state how you assessed strength. Specifically, you should use $r$ to assess strength (see ) IF the relationship is linear wihout any outliers. However, if the relationship is nonlinear, has outliers, or both, then strength should be subjectively assessed from the scatterplot.

Two other points to consider when performing a bivariate EDA with quantitative variables. First, if outliers are present, do not let them completely influence your conclusions about form, association, and strength. In other words, try to assess these items while ignoring the outlier(s). If you have the raw data and the form excluding the outlier is linear, then compute $r$ with the outlier eliminated from the data. Second, the form of weak relationships is difficult to describe because, by definition, there is very little clustering to a form. As a rule-of-thumb, if the scatterplot does not have an obvious curvature to it, then it is described as linear by default.

\warn{Outliers should not influence the descriptions of association, strength, and form.}

\vspace{-12pt}
\warn{The form is linear unless there is an OBVIOUS curvature.}


\subsection*{Highway MPG and Weight}
The following overall bivariate summary for the relationship between highway MPG and weight is made using the calculations from the previous sections. The relationship between highway MPG and weight of cars \figrefp{fig:carscat2} appears to be primarily linear (although I see a very slight concavity), negative, and moderately strong with a correlation of \Sexpr{formatC(corr(HMPG~Weight,data=cars93),format="f",digits=2)}. The three points at (2400,46), (2500,27), and (1800,33) might be considered SLIGHT outliers (these are not far enough removed for me to consider them outliers, but some people may). The correlation coefficient was used to assess strength because the relationship was linear without any outliers.


\subsection*{State Energy Usage}
\begin{quote}
\textit{A 2001 report from the \href{http://www.eia.doe.gov/}{Energy Information Administration} of the Department of Energy details the total consumption of a variety of energy sources by state in 2001. Construct a proper EDA for the relationship between total petroleum and coal consumption (in trillions of BTU).}
\end{quote}
<<NRG1, echo=FALSE>>=
NRG <- read.csv("data/NRG_Consump_2001.csv")
NRG1 <- NRG[-c(5,44),]
@
The relationship between total petroleum and coal consumption is generally positive, linear, weak, with two outliers at total petroleum levels greater than 3000 trillions of BTU (\figref{fig:scatNRG1}-Left). I did not compute a correlation coefficient because of the outliers. The two outliers were Texas and California. After removing them from the data set the relationship is clearly positive, linear, weak ($r=$\Sexpr{formatC(corr(~Coal+TotalPet,data=NRG1),format="f",digits=2)}), with no additional outliers (\figref{fig:scatNRG1}-Right).

<<scatNRG1, echo=FALSE, fig.cap="Scatterplot of the total consumption of petroleum versus the consumption of coal (in trillions of BTU) by all 50 states and the District of Columbia. The points shown in the left with total petroleum values greater than 3000 trillion BTU are deleted in the right plot.">>=
plot(TotalPet~Coal,data=NRG,pch=16,xlab="Coal Consumption (trillion BTU)",
     ylab="Total Petroleum (trillion BTU)")
plot(TotalPet~Coal,data=NRG1,pch=16,xlab="Coal Consumption (trillion BTU)",
     ylab="Total Petroleum (trillion BTU)")
@

\subsubsection*{R Appendix}
<<eval=FALSE, prompt=FALSE>>=
<<NRG1>>
<<scatNRG1>>
corr(~Coal+TotalPet,data=NRG1)
@

\newpage
\subsection*{Hatch Weight and Incubation Time of Geckos}
\begin{quote}
\textit{A hobbyist hypothesized that there would be a positie association between length of incubation (days) and hatchling weigth (grams) for Crested Geckos. To test this hypothesis she collected the incubation time and weight for 21 hatchlings (shown below). Construct a proper EDA for the relationship between incubation time and hatchling weight.}
\end{quote}

\begin{verbatim}
Time  53  54  56  60  60  60  60  60  63  63  77  77  78  81  82  82  83  83  84  90  90
Wt   1.5 1.7 1.4 1.0 1.4 1.5 1.7 1.8 1.4 1.5 1.1 1.6 1.5 1.9 1.4 1.5 1.3 1.7 1.6 1.4 1.8
\end{verbatim}

<<echo=FALSE>>=
df <- data.frame(inctime=c(53,54,56,60,60,60,60,60,63,63,77,77,78,81,82,82,83,83,84,90,90),hatchwt=c(1.5,1.7,1.4,1.0,1.4,1.5,1.7,1.8,1.4,1.5,1.1,1.6,1.5,1.9,1.4,1.5,1.3,1.7,1.6,1.4,1.8))
@

The relationship between hatchling weight and incubation time for the Crested Geckos shows a linear form without outliers, no definitive and a weak ($r$=\Sexpr{formatC(corr(~inctime+hatchwt,data=df),format="f",digits=2)}) association \figrefp{fig:scatGecko}. I did compute $r$ because no outliers were present and the relationship was linear (or, at least, it was not nonlinear).

<<scatGecko, echo=FALSE, fig.cap="Scatterplot of hatchling weight versus incubation time for Crested Geckos.">>=
plot(hatchwt~inctime,data=df,pch=16,xlab="Incubation Time (days)",
     ylab="Hatchling Weight (grams)")
@

\subsubsection*{R Appendix}
<<eval=FALSE, prompt=FALSE>>=
df <- read.csv("data/Gecko.csv")
<<scatGecko>>
corr(~inctime+hatchwt,data=df)
@


\section{Cautions About Correlation}
Examning relationships between pairs of quantitative variables is common practice. Using the $r$ can be an important part of this analysis, as described above. However, $r$ can be abused through misapplication and misinterpretation. Thus, it is important to remember the following characteristics of correlation coefficients:
\begin{Itemize}
  \item Variables must be quantitative (i.e., if you cannot make a scatterplot, then you cannot calculate $r$).
  \item The correlation coefficient only measures strength of \textbf{LINEAR} relationships (i.e., if the form of the relationship is not linear, then $r$ is meaningless and should not be calculated).
  \item The units that the variables are measured in do not matter (i.e., $r$ is the same between heights and weights measured in inches and lbs, inches and kg, m and kg, cm and kg, and cm and inches). This is because the variables are standardized in the calculation of $r$.
  \item The distinction between response and explanatory variables is not needed. That is, the correlation of GPA and ACT scores is the same as the correlation of ACT scores and GPA.
  \item Correlation coefficients are between -1 and 1.
  \item Correlation coefficients are strongly affected by outliers (simply, because both the mean and standard deviation, used in the calculation of $r$, are strongly affected by outliers).
\end{Itemize}

Additionally, correlation is not causation! In other words, just because a strong correlation is observed it doesn't mean that the explanatory variable caused the response variable (an exception may be in carefully designed experimental studies). For example, a negative relationship between a car's weight and highway gas mileage was found above. Thus, it appears that as the weight of the car increased, the highway gas mileage decreased linearly. While this conclusion is correct, it is also very carefully worded. One must be careful to not state that increasing the weight of the car CAUSES a decrease in MPG. We cannot attribute cause because these data come from an observational study and because several other important variables were not considered in the analysis. For example, the scatterplot in \figref{fig:carscat3}, coded for different numbers of cylinders in the car's engine, indicates that the number of cylinders may be inversely related to highway MPG and positively related to weight of the car. So, does the weight of the car, the number of cylinders, or both, explain the decrease in highway MPG?

<<carscat3, echo=FALSE, fig.cap="Scatterplot between the highway MPG and weight of cars manufactured in 1993 separated by number of cylinders.">>=
plot(HMPG~Weight,data=cars93,pch=Cyls-2,xlab="Weight (lbs)",ylab="Highway MPG")
legend("topright",pch=1:5,legend=c("3-cyl","4-cyl","5-cyl","6-cyl","8-cyl"))
@

More interesting examples (i.e., high correlation between number of people who drowned by falling into a pool and the annual number of films that Nicolas Cage appeared in) the demonstrate that ``correlation is not causation'' can be found on the \href{http://www.tylervigen.com/spurious-correlations}{Spurious Correlations website.}

Finally, the word ``correlation'' is often mis-used in everyday language. This word should only be used when discussing the actual correlation coefficient (i.e., $r$). When discussing the association between two variables, one should use the word ``relationship'' rather than ``correlation.'' For example, one might ask ``What is the relationship between age and rate of cancer?'', but should not ask ``What is the correlation between age and rate of cancer?''.
