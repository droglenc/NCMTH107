<<echo=FALSE, cache=FALSE, results='hide'>>=
set_parent('IntroStats.Rnw')
@

\chapter{Linear Regression}  \label{chap:Regress}

\minitoc
\vspace{48pt}

\lettrine{L}{inear regression analysis is used to model the relationship} between two quantitative variables for two related purposes -- (i) explaining variability in the response variable and (ii) predicting future values of the response variable. Examples include predicting future sales of a product from its price, family expenditures on recreation from family income, an animal's food consumption in relation to ambient temperature, and a person's score on a German assessment test based on how many years the person studied German.

Exact predictions cannot be made because of natural variability. For example, two people with the same intake of mercury (from consumption of fish) will likely not have the same level of mercury in their blood stream. Thus, the best that can be accomplished is to predict the average or expected value for a person with a particular intake value. This is accomplished by finding the line that best ``fits'' the points on a scatterplot of the data and using that line to make predictions. Finding and using the ``best-fit'' line is the topic of this module.


\section{Response and Explanatory Variables}
Recall from \sectref{sect:RespExplan1} that the response (or dependent) variable is the variable to be predicted or explained and the explanatory (or independent) variable is the variable that will help do the predicting or explaining. In the examples mentioned above, future sales, family expenditures on recreation, the animal's food consumption, and score on the assessment test are response variables and product price, family income, temperature, and years studying German are explanatory variables, respectively. The response variable is on the y-axis and the explanatory variable is on the x-axis of scatterplots.


\section{Slope and Intercept}
The equation of a line is commonly expressed as,
  \[ y = mx + b  \]
where both $x$ and $y$ are variables, $m$ represents the slope of the line, and $b$ represents the y-intercept.\footnote{Hereafter, simply called the ``intercept.''}  It is important that you can look at the equation of a line and identify the response variable, explanatory variable, slope, and intercept. The response variable will always appear on the left side of the equation by itself. The explanatory variable (e.g., $x$) will be on the right side of the equation and will be multiplied by the slope. The value or symbol by itself on the right side of the equation is the intercept. For example, in
\[ blood = 3.501 + 0.579intake \]
\var{blood} is the response variable, \var{intake} is the explanatory variable, $0.579$ is the slope (it is multiplied by the explanatory variable), and $3.501$ is the intercept (it is on the right side and not multiplied by anything). The same conclusions would be made if the equation had been written as
  \[ blood = 0.579intake+3.501 \]

\warn{In the equation of a line, the slope is always multiplied by the explanatory variable and the intercept is always by itself.}

In addition to being able to identify the slope and intercept of a line you also need to be able to interpret these values. Most students define the slope as ``rise over run'' and the intercept as ``where the line crosses the y-axis.''  These ``definitions'' are loose geometric representations. For our purposes, the slope and intercept must be more strictly defined.

To define the slope, first think of ``plugging'' two values of intake into the equation discussed above. For example, if $intake=100$, then $blood=3.501+0.579*100$=\Sexpr{formatC(3.501+0.579*100,format="f",digits=2)} and if $intake$ is one unit larger at $101$, then $blood=3.501+0.579*101$=\Sexpr{formatC(3.501+0.579*101,format="f",digits=2)}.\footnote{For simplicity of exposition, the actual units are not used in this discussion. However, ``units'' would usually be replaced with the actual units used for the measurements.} The difference between these two values is \Sexpr{formatC(3.501+0.579*101,format="f",digits=2)}-\Sexpr{formatC(3.501+0.579*100,format="f",digits=2)}=$0.579$, which is the same as the slope. Thus, the slope is the change in value of the response variable FOR A SINGLE UNIT CHANGE in the value of the explanatory variable \figrefp{fig:SlopeInt}. That is, mercury in the blood changes 0.579 units FOR A SINGLE UNIT CHANGE in mercury intake. So, if an individual increases mercury intake by one unit, then mercury in the blood will increase by 0.579 units, ON AVERAGE. Alternatively, if one individual has one more unit of mercury intake than another individual, then the first individual will have 0.579 more units of mercury in their blood, ON AVERAGE.

<<SlopeInt, echo=FALSE,fig.width=4,fig.height=4, fig.cap="Schematic representation of the meaning of the intercept and slope in a linear equation.">>=
int <- 1.5
slope <- 0.9
ggplot() +
  geom_abline(intercept=int,slope=slope,size=1.5) +
  scale_x_continuous(limits=c(0,10),breaks=0,expand=expansion(mult=c(0,0))) +
  scale_y_continuous(limits=c(0,10),breaks=c(0,int),labels=c(0,"intercept"),
                     expand=expansion(mult=c(0,0))) +
  labs(x="Explanatory Variable",y="Response Variable") +
  annotate(geom="segment",x=2,xend=3,y=int+slope*2,yend=int+slope*2,
           color="red",size=1,lineend="square") +
  annotate(geom="segment",x=3,xend=3,y=int+slope*2,yend=int+slope*3,
           color="red",size=1,lineend="square") +
  annotate(geom="text",x=2.5,y=int+slope*2,label="1",vjust=1.2,
           color="red",size=4) +
  annotate(geom="text",x=3,y=int+slope*2.5,label="slope",hjust=-0.2,vjust=0.4,
           color="red",size=4) +
  annotate(geom="segment",x=6,xend=7,y=int+slope*6,yend=int+slope*6,
           color="red",size=1,lineend="square") +
  annotate(geom="segment",x=7,xend=7,y=int+slope*6,yend=int+slope*7,
           color="red",size=1,lineend="square") +
  annotate(geom="text",x=6.5,y=int+slope*6,label="1",vjust=1.2,
           color="red",size=4) +
  annotate(geom="text",x=7,y=int+slope*6.5,label="slope",hjust=-0.2,vjust=0.4,
           color="red",size=4) +
  theme_NCStats() +
  theme(aspect.ratio=1)
@

To define the intercept, first ``plug'' $intake=0$ into the equation discussed above; i.e., $blood=3.501+0.579*0$ = $3.501$. Thus, the intercept is the value of the response variable when the explanatory variable is equal to zero \figrefp{fig:SlopeInt}. In this example, the AVERAGE mercury in the blood for an individual with no mercury intake is 3.501. Many times, as is true with this example, the interpretation of the intercept will be nonsensical. This is because $x=0$ will likely be outside the range of the data collected and, perhaps, outside the range of possible data that could be collected.

The equation of the line is a model for the relationship depicted in a scatterplot. Thus, the interpretations for the slope and intercept represent the \textit{average} change or the \textit{average} response variable. Thus, whenever a slope or intercept is being interpreted it must be noted that the result is an \textit{average} or \textit{on average}.


\section{Predictions}
Once a best-fit line has been identified (criteria for doing so is discussed in \sectref{sect:BestFitLine}), the equation of the line can be used to predict the average value of the response variable for individuals with a particular value of the explanatory variable. For example, the best-fit line for the mercury data shown in \figref{fig:HGscat} is
  \[ blood = 3.501 + 0.579*intake \]
Thus, the predicated average level of mercury in the blood for an individual that consumed 240 ug HG/day is found with
  \[ blood = 3.501 + 0.579*240 = 142.461 \]
Similarly, the predicted average level of mercury in the blood for an individual that consumed 575 ug HG/day is found with
  \[ blood = 3.501 + 0.579*575 = 336.426 \]
A prediction may be visualized by finding the value of the explanatory variable on the x-axis, drawing a vertical line until the best-fit line is reached, and then drawing a horizontal line over to the y-axis where the value of the response variable is read \figrefp{fig:HGpredict}.

<<HGpredict, echo=FALSE, message=FALSE, fig.cap="Scatterplot between the intake of mercury in fish and the mercury in the blood stream of individuals with superimposed best-fit regression line illustrating predictions for two values of mercury intake.">>=
merc <- data.frame(intake=c(180,200,230,410,600,550,275,580,580,105,250,60,650),
                   blood=c(90,120,125,290,310,290,170,275,350,70,105,75,480))
lm1 <- lm(blood~intake,data=merc)
df1 <- c(240,575)
p1 <- predict(lm1,data.frame(intake=df1))
p1l <- formatC(p1,format="f",digits=1)
tmp <- data.frame(x=rep(df1,each=2),xend=c(rbind(df1,c(-Inf,-Inf))),
                  y=c(rbind(c(-Inf,-Inf),p1)),yend=rep(p1,each=2))

ggplot(data=merc,mapping=aes(x=intake,y=blood)) +
  geom_point(pch=21,color="black",fill="darkgray",size=2) +
  geom_smooth(method="lm",se=FALSE,color="red",size=1) +
  labs(x="Intake (ug Hg/day)",y="Blood (ng/g)") +
  scale_x_continuous(breaks=c(100,200,df1[1],300,400,500,df1[2],600),
                     labels=c(100,"",df1[1],"",400,500,df1[2],"")) +
  scale_y_continuous(breaks=c(100,p1[1],200,300,p1[2],400,500),
                     labels=c(100,p1l[1],200,300,p1l[2],400,500)) +
  geom_segment(data=tmp,mapping=aes(x=x,xend=xend,y=y,yend=yend),
               color="red",linetype="dashed") +
  theme_NCStats()
@

\vspace{12pt} %because it was gobbled up
When predicting values of the response variable, it is important to not extrapolate beyond the range of the data. In other words, predictions with values outside the range of observed values of the explanatory variable should be made cautiously (if at all). An excellent example would be to consider height ``data'' collected during the early parts of a human's life (say the first ten years). During these early years there is likely a good fit between height (the response variable) and age. However, using this relationship to predict an individual's height at age 40 would likely result in a ridiculous answer (e.g., over ten feet). The problem here is that the linear relationship only holds for the observed data (i.e., the first ten years of life); it is not known if the same linear relationship exists outside that range of years. In fact, with human heights, it is generally known that growth first slows, eventually quits, and may, at very old ages, actually decline. Thus, the linear relationship found early in life does not hold for later years. Critical mistakes can be made when using a linear relationship to extrapolate outside the range of the data.

\section{Residuals}
The predicted value is a ``best-guess'' for an individual based on the best-fit line. The actual value for any individual is likely to be different from this predicted value. The \textbf{residual} is a measure of how ``far off'' the prediction is from what is actually observed. Specifically, the residual for an individual is found by subtracting the predicted value (given the individual's observed value of the explanatory variable) from the individual's observed value of the response variable, or
  \[ \text{residual}=\text{observed response}-\text{predicted response} \]

For example, consider an individual that has an observed intake of 650 and an observed level of mercury in the blood of 480. As shown in the previous section, the predicted level of mercury in the blood for this individual is
  \[ blood = 3.501 + 0.579*650 = 379.851 \]

The residual for this individual is then $480-379.851$ = $100.149$. This positive residual indicates that the observed value is approximately 100 units GREATER than the average for individuals with an intake of 650.\footnote{In other words, the observed value is ``above'' the line.}  As a second example, consider an individual with an observed intake of 250 and an observed level of mercury in the blood of 105. The predicted value for this individual is
  \[ blood = 3.501 + 0.579*250 = 148.251 \]

and the residual is $105-148.251$ = $-43.251$. This negative residual indicates that the observed value is approximately 43 units LESS than the average for individuals with an intake of 250.

Visually, a residual is the vertical distance between an individual's point and the best-fit line \figrefp{fig:HGresidual}.

<<HGresidual, echo=FALSE, message=FALSE, fig.cap="Scatterplot between the intake of mercury in fish and the mercury in the blood stream of individuals with superimposed best-fit regression line illustrating the residuals for the two individuals discussed in the main text.">>=
tmp <- merc[c(11,13),] %>%
  mutate(xend=intake,yend=predict(lm1,data.frame(intake=intake)),
         resid=c("Neg","Pos"))

ggplot(data=merc,mapping=aes(x=intake,y=blood)) +
  geom_point(pch=21,color="black",fill="darkgray",size=2) +
  geom_smooth(method="lm",se=FALSE,color="red",size=1) +
  labs(x="Intake (ug Hg/day)",y="Blood (ng/g)") +
  scale_x_continuous(breaks=seq(100,600,100)) +
  scale_y_continuous(breaks=seq(100,500,100)) +
  geom_segment(data=tmp,mapping=aes(x=intake,xend=xend,y=blood,yend=yend,color=resid),
               linetype="dotted",size=1) +
  scale_color_manual(values=c("blue","red")) +
  theme_NCStats() +
  theme(legend.position="none")
@

\section{Best-fit Criteria}\label{sect:BestFitLine}
An infinite number of lines can be placed on a graph, but many of those lines do not adequately describe the data. In contrast, many of the lines will appear, to our eye, to adequately describe the data. So, how does one find THE best-fit line from all possible lines. The \textbf{least-squares} method described below provides a quantifiable and objective measure of which line best ``fits'' the data.

Residuals are a measure of how far an individual is from a candidate best-fit line. Residuals computed from all individuals in a data set measure how far all individuals are from the candidate best-fit line. Thus, the residuals for all individuals can be used to identify the best-fit line.

The residual sum-of-squares (RSS) is the sum of all squared residuals. The least-squares criterion says that the ``best-fit'' line is the one line out of all possible lines that has the minimum RSS \figrefp{fig:RSSanim}.

<<RSSanim, echo=FALSE, cache=TRUE, fig.width=7, fig.height=3.5, fig.show='animate',fig.cap="An animation illustrating how the residual sum-of-squares (RSS) for a series of candidate lines (red lines) is minimized at the best-fit line (green line).", aniopts='controls,palindrome,autoplay',out.width='.8\\linewidth'>>=
################################################################################
# Function to calculate the RSS given a dataframe that must have the explanatory
#   variable in the first column and the response variable in the second column
#   and a dataframe that contains the paired parameter estimates with the slopes
#   in the first column and the intercepts in the second column. Returns a data
#   frame with the slopes as the first, the intercepts as the second, and the
#   RSS as the third column.
################################################################################
RSS2 <- function(d,ests) {
  RSS <- NULL                               # Initiate the vector
  for (i in 1:nrow(ests)) {                 # Cycle throught the possible parameter estimates
    pred <- ests[i,1]*d[,1]+ests[i,2]       # Predicted values for current parameter estimates
    RSS <- c(RSS,sum((d[,2]-pred)^2))       # Put the RSS for the current parameter estimates onto the end of the vector
  }
  data.frame(ests,RSS)
}

################################################################################
# A function that receives a dataframe that has the sequence of parameter
#   estimates in the first two columns and the corresponding RSS in the third
#   column. In addition, the index position of the current potential parameters
#   must be sent. This function then plots the RSS (e.g., RSS vs. an index)
#   and a red vertical line and dot at the current estimate.
################################################################################
plotRSS<-function(r,icurr) {
  # Creates an index
  i <- seq(1,nrow(r))
  # Plot the function
  plot(i,r[,3],type="l",lwd=2,main="",xlab="",ylab="RSS",xaxt="n",yaxt="n")
  # Labels x-axis
  mtext(paste("slope","intercept",sep=","),1,1)
  # Put line at current guess
  lines(c(icurr,icurr),c(min(r[,3]),max(r[,3])),lwd=2,col="red")
  # Put point at current guess
  points(icurr,r[icurr,3],pch=21,col="red",bg="gray70",cex=1.2)
}

################################################################################
# Plots the raw data with residuals to the current estimate
################################################################################
plotEst<-function(d,e,icurr,...) {
  # Plot the data
  plot(d[,1],d[,2],xlab=names(d)[1],ylab=names(d)[2],pch=21,bg="gray70",...)
  # Fit the linear model
  l1<-lm(d[,2]~d[,1])
  # Plot the best-fit line in green
  abline(coef(l1),lwd=1,col="green")
  # Plot current candidate line in red
  abline(c(e[icurr,2],e[icurr,1]),lwd=2,col="red")
  # Put on the residuals to the current estimate
  for (i in 1:nrow(d)) {
    pred <- e[icurr,1]*d[i,1]+e[icurr,2]
    clr <- ifelse((d[i,2]-pred)<0,"blue","red")
    lines(c(d[i,1],d[i,1]),c(d[i,2],pred),lty=2,lwd=2,col=clr)
  }
}

# create intercepts
ints <- seq(-180,190,5)
# create slopes from the intercepts given the mean of x and mean of y
xbar <- mean(merc$intake)
ybar <- mean(merc$blood)
slps <- sapply(ints,FUN=function(int,xbar,ybar) (ybar-int)/xbar, xbar=xbar,ybar=ybar)
params <- data.frame(slps,ints)


par(mar=c(3.5,3.5,1,1),mgp=c(2,0.75,0),mfrow=c(1,2))
# Compute RSS values for all parameter choices
RSSs <- RSS2(merc,params)
# Cycle through the estimates
for(i in 1:nrow(params)) {
  # Make the fitted plot
  plotEst(merc,params,i,ylim=c(-100,600),xlim=c(0,700))
  # Make the RSS plot
  plotRSS(RSSs,i)
}
@

\vspace{12pt} %because it got gobbled up

The discussion thusfar implies that all possible lines must be ``fit'' to the data and the one with the minimum RSS is chosen as the ``best-fit'' line. As there are an infinite number of possible lines, this would be impossible to do. Theoretical statisticians have shown that the application of the least-squares criterion always produces a best-fit line with a slope given by
  \[ slope = r\frac{s_{y}}{s_{x}}  \]

and an intercept given by
  \[ intercept = \bar{y}-slope*\bar{x}   \]

where $\bar{x}$ and $s_{x}$ are the sample mean and standard deviation of the explanatory variable, $\bar{y}$ and $s_{y}$ are the sample mean and standard deviation of the response variable, and $r$ is the sample correlation coefficient between the two variables. Thus, using these formulas finds the slope and intercept for the line, out of all possible lines, that minimizes the RSS.


\section{Assumptions}\label{sect:RegAssumptions}
The least-squares method for finding the best-fit line only works appropriately if each of the following five assumptions about the data has been met.

\begin{Enumerate}
  \item A line describes the data (i.e., a linear form).
  \item Homoscedasticity.
  \item Normally distributed residuals at a given x.
  \item Independent residuals at a given x.
  \item The explanatory variable is measured without error.
\end{Enumerate}

While all five assumptions of linear regression are important, only the first two are vital when the best-fit line is being used primarily as a descriptive model for data.\footnote{In contrast to using the model to make inferences about a population model.}  Description is the primary goal of linear regression used in this course and, thus, only the first two assumptions are considered further.

The linearity assumption appears obvious -- if a line does not represent the data, then don't try to fit a line to it!  Violations of this assumption are evident by a non-linear or curving form in the scatterplot.

The homoscedasticity assumption states that the variability about the line is the same for all values of the explanatory variable. In other words, the dispersion of the data around the line must be the same along the entire line. Violations of this assumption generally present as a ``funnel-shaped'' dispersion of points from left-to-right on a scatterplot.

Violations of these assumptions are often evident on a ``fitted-line plot'', which is a scatterplot with the best-fit line superimposed \figrefp{fig:ResidPlotEx}.\footnote{Residual plots, not discussed in this text, are another plot that often times is used to better assess assumption violations.} If the points look more-or-less like random scatter around the best-fit line, then neither the linearity nor the homoscedasticity assumption has been violated. A violation of one of these assumptions should be obvious on the scatterplot. In other words, there should be a clear curvature or funneling on the plot.

<<ResidPlotEx, echo=FALSE, fig.width=6, fig.height=6, out.width='.75\\linewidth', fig.cap="Fitted-line plots illustrating when the regression assumptions are met (upper-left) and three common assumption violations.">>=
par(mar=c(2,3,3,2),mgp=c(0.5,0,0),mfrow=c(2,2),xaxt="n",yaxt="n")
set.seed(101)
n <- 100
mu <- 0
sigma <- 10
slp <- 1
int <- 0
x <- rnorm(n,mu,sigma)
e1 <- rnorm(n,mu,sigma)
y1 <- slp*x+int+e1
lm1 <- lm(y1~x)
fitPlot(lm1,xlab="Explanatory",ylab="Response",pch=21,bg="gray70")
mtext("Assumptions Met",line=0.25,col="blue")

e2 <- ((x-mean(x))/5)^2 + rnorm(n,0,sigma/4)
y2 <- slp*x+int+e2
lm2 <- lm(y2~x)
fitPlot(lm2,xlab="Explanatory",ylab="Response",pch=21,bg="gray70")
mtext("Non-Linear",line=0.25,col="red")

x1 <- runif(n,min=0,max=10)
e3 <- rep(0,n)
for (i in 1:n) e3[i] <- rnorm(1,0,x1[i]/2)
y3 <- slp*x1+int+e3
lm3 <- lm(y3~x1)
fitPlot(lm3,xlab="Explanatory",ylab="Response",pch=21,bg="gray70")
mtext("Heteroscedastic",line=0.25,col="red")

y4 <- 2*log(x1) + rnorm(n,0,0.4)
y4 <- exp(y4)
lm4 <- lm(y4~x1)
fitPlot(lm4,xlab="Explanatory",ylab="Response",pch=21,bg="gray70")
mtext("Non-linear & Heteroscedastic",line=0.25,col="red")
@

In this course, if an assumption has been violated, then one should not continue to interpret the linear regression. However, in many instances, an assumption violation can be ``corrected'' by transforming one or both variables to a different scale. Transformations are not discussed in this course.

\warn{If the regression assumptions are not met, then the regression results should not be interpreted.}


\section{Coefficient of Determination}
The coefficient of determination ($r^{2}$) is the proportion of the total variability in the response variable that is explained away by knowing the value of the explanatory variable and the best-fit model. In simple linear regression, $r^{2}$ is literally the square of $r$, the correlation coefficient.\footnote{Simple linear regression is the fitting of a model with a single explanatory variable and is the only model considered in this module and this course. See \sectref{sect:corr} for a review of the correlation coefficient.} Values of $r^{2}$ are between 0 and 1.\footnote{It is common for $r^{2}$ to be presented as a percentage.}

The meaning of $r^{2}$ can be examined by making predictions of the response variable with and without knowing the value of the explanatory variable. First, consider predicting the value of the response variable without any information about the explanatory variable. In this case, the best prediction is the sample mean of the response variable (represented by the dashed blue horizontal line in \figref{fig:CoeffDeterm}). However, because of natural variability, not all individuals will have this value. Thus, the prediction might be ``bracketed'' by predicting that the individual will be between the observed minimum and maximum values (solid blue horizontal lines). Loosely speaking, this range is the ``total variability in the response variable'' (blue box).

<<CoeffDeterm, echo=FALSE, fig.width=6, fig.height=3.5, out.width='.8\\linewidth', fig.cap="Fitted line plot with visual representations of variabilities explained and unexplained. A full explanation is in the text.">>=
## Makes some data
set.seed(199)
covmat <- matrix(c(1,0.9,0.9,1),ncol=2)
x <- rmvnorm(n=30,mean=c(0,0),sigma=covmat)
y <- 3*x[,2]+80
x <- 3*x[,1]+50
miny <- which.min(y)
maxy <- which.max(y)
avgy <- mean(y)
lm1 <- lm(y~x)
predx <- 52
predy <- predict(lm1,data.frame(x=predx))

## Makes Scatterplot
layout(matrix(1:2,nrow=1),widths=c(3,1))
par(mar=c(3,3,0.5,0),mgp=c(1.9,0.5,0),tcl=-0.2,xaxs="i",yaxs="i")
# base plot
plot(y~x,pch=21,bg="gray70",bty="l",ylim=c(74.5,85),xlim=c(44.5,55))
abline(lm1,col="gray30",lwd=2)
# total variability lines (and line at mean)
lines(c(x[miny],60),c(y[miny],y[miny]),col="darkblue",lwd=2)
lines(c(x[maxy],60),c(y[maxy],y[maxy]),col="darkblue",lwd=2)
lines(c(44.5,54),c(avgy,avgy),col="darkblue",lwd=2,lty=2)
# variability remaining lines (and line at prediction)
lines(c(predx,predx),c(predy,74.5),col="darkred",lwd=2,lty=2)
lines(c(predx,44.5),c(predy,predy),col="darkred",lwd=2,lty=2)
predymin <- predy-1.9
predymax <- predy+1.9
lines(c(predx,60),c(predymin,predymin),col="darkred",lwd=2)
lines(c(predx,60),c(predymax,predymax),col="darkred",lwd=2)


## Makes variability bars
par(mar=c(3,0,0.5,0))
plot(1,xlim=c(0,3),ylim=c(74.5,85),col="white",bty="n",xlab="",xaxt="n",yaxt="n")
# variability explained
rect(0.05,y[miny],0.95,predymin,col="darkgreen",border="darkgreen")
rect(0.05,predymax,0.95,y[maxy],col="darkgreen",border="darkgreen")
text(0.5,74.5+(predymin-74.5)/2,"Variability\nExplained",col="white",cex=1.05,font=2,srt=90)
# variability remaining lines
arrows(0,predymin,1.05,predymin,lwd=2,col="darkred",length=0.1)
arrows(0,predymax,1.05,predymax,lwd=2,col="darkred",length=0.1)
rect(1.05,predymin,1.95,predymax,col="darkred",border="darkred")
text(1.5,predy,"Variability\nRemains",col="white",cex=1.05,font=2,srt=90)
# total variability lines
arrows(0,y[miny],2.05,y[miny],lwd=2,col="darkblue",length=0.1)
arrows(0,y[maxy],2.05,y[maxy],lwd=2,col="darkblue",length=0.1)
rect(2.05,y[miny],2.95,y[maxy],col="darkblue",border="darkblue")
text(2.5,avgy,"Total Variability in Y",col="white",cex=1.1,font=2,srt=90)
@

Suppose now that the response variable is predicted for an individual with a known value of the explanatory variable (e.g., at the dashed vertical red line in \figref{fig:CoeffDeterm}). The predicted value for this individual is the value of the response variable at the corresponding point on the best-fit line (dashed horizontal red line). Again, because of natural variability, not all individuals with this value of the explanatory variable will have this exact value of the response variable. However, the prediction is now ``bracketed'' by the minimum and maximum value of the response variable \textbf{ONLY} for those individuals with the same value of the explanatory variable (solid red horizontal lines). Loosely speaking, this range is the ``variability in the response variable remaining after knowing the value of the explanatory variable'' (red box). This is the variability in the response variable that remains even after knowing the value of the explanatory variable or the variability in the response variable that cannot be explained away (by the explanatory variable).

The portion of the total variability in the response variable that was explained away consists of all the values of the response variable that would no longer be entertained as possible predictions once the value of the explanatory variable is known (green box in \figref{fig:CoeffDeterm}).

Now, by the definition of $r^{2}$, $r^{2}$ can be visualized as the area of the green box divided by the area of the blue box. This calculation does not depend on which value of the explanatory variable is chosen as long as the data are evenly distributed around the line (i.e., homoscedasticity -- see \sectref{sect:RegAssumptions}).

If the variability explained away (green box) approaches the total variability in the response variable (blue box), then $r^{2}$ approaches 1. This will happen only if the variability around the line approaches zero. In contrast, the variability explained (green box) will approach zero if the slope is zero (i.e., no relationship between the response and explanatory variables). Thus, values of $r^{2}$ also indicate the strength of the relationship; values near 1 are stronger than values near 0. Values near 1 also mean that predictions will be fairly accurate -- i.e., there is little variability remaining after knowing the explanatory variable.

\warn{A value of $r^{2}$ near 1 represents a strong relationship between the response and explanatory variables that will lead to accurate predictions.}


\section{Examples}
There are twelve questions that are commonly asked about linear regression results. These twelve questions are listed below with some hints about things to remember when answering some of the questions. An example of these questions in context is then provided.

\begin{enumerate}
  \item What is the response variable?  \textit{Identify which variable is to be predicted or explained, which variable is dependent on another variable, which would be hardest to measure, or which is on the y-axis.}
  \item What is the explanatory variable?  \textit{The remaining variable after identifying the response variable.}
  \item Comment on linearity and homoscedasticity. \textit{Examine fitted-line plot for curvature (i.e., non-linearity) or a funnel-shape (i.e., heteroscedasticity).}
  \item What is the equation of the best-fit line?  \textit{In the generic equation of the line ($y=mx+b$) replace $y$ with the name of the response variable, $x$ with the name of the explanatory variable, $m$ with the value of the slope, and $b$ with the value of the intercept.}
  \item Interpret the value of the slope. \textit{Comment on how the response variable changes by slope amount for each one unit change of the explanatory variable, on average.}
  \item Interpret the value of the intercept. \textit{Comment on how the response variable equals the intercept, on average, if the explanatory variable is zero.}
  \item Make a prediction given a value of the explanatory variable. \textit{Plug the given value of the explanatory variable into the equation of the best-fit line. Make sure that this is not an extrapolation.}
  \item Compute a residual given values of both the explanatory and response variables. \textit{Make a prediction (see previous question) and then subtract this value from the observed value of the response. Make sure that the prediction is not an extrapolation.}
  \item Identify an extrapolation in the context of a prediction problem. \textit{Examine the x-axis scale on the fitted-line plot and do not make predictions outside of the plotted range.}
  \item What is the proportion of variability in the response variable explained by knowing the value of the explanatory variable?  \textit{This is $r^{2}$.}
  \item What is the correlation coefficient?  \textit{This is the square root of $r^{2}$. Make sure to put a negative sign on the result if the slope is negative.}
  \item How much does the response variable change if the explanatory variable changes by X units?  \textit{This is an alternative to asking for an interpretation of the slope. If the explanatory variable changes by X units, then the response variable will change by X*slope units, on average.}
\end{enumerate}

All answers should refer to the variables of the problem -- thus, ``y'', ``x'', ``response'', or ``explanatory'' should not be in any part of any answer. The questions about the slope, intercept, and predictions need to explicitly identify that the answer is an ``average'' or ``on average.''

\warn{All interpretations should be ``in terms of the variables of the problem'' rather than the generic terms of x, y, response variable, and explanatory variable.}


\vspace{24pt}
\subsection*{Chimp Hunting Parties}
\begin{quote}
\textit{Stanford (1996) gathered data to determine if the size of the hunting party (number of individuals hunting together) affected the hunting success of the party (percent of hunts that resulted in a kill) for wild chimpanzees (Pan troglodytes) at Gombe. The results of their analysis for 17 hunting parties is shown in the figure below.\footnote{These data are in \href{https://raw.githubusercontent.com/droglenc/NCData/master/Chimp.csv}{Chimp.csv}.}  Use these results to answer the questions below.}
\end{quote}

<<ChimpFLP, echo=FALSE, message=FALSE>>=
chimp <- read.csv("https://raw.githubusercontent.com/droglenc/NCData/master/Chimp.csv")
lm.chimp <- lm(percsucc~huntparty,data=chimp)
slp <- formatC(coef(lm.chimp)[2],format="f",digits=1)
int <- formatC(coef(lm.chimp)[1],format="f",digits=0)
r2 <- formatC(rSquared(lm.chimp),format="f",digits=2)
ggplot(data=chimp,mapping=aes(x=huntparty,y=percsucc)) +
  geom_point(pch=21,color="black",fill="darkgray",size=2) +
  geom_smooth(method="lm",se=FALSE,color="red",size=1) +
  ggpubr::stat_regline_equation(aes(label=..eq.label..),
                                label.x.npc=0,label.y.npc=1) +
  ggpubr::stat_regline_equation(aes(label=..rr.label..),
                                label.x.npc=0,label.y.npc=0.90) +
  labs(x="Number of Hunting Party Members",y="% Successful Hunts") +
  theme_NCStats() +
  theme(aspect.ratio=1,plot.title=element_text(size=12))
@

\begin{QAlist}
  \item What is the response variable?
  \begin{QAlist}
    \item The response variable is the percent of successful hunts because the authors are attempting to see if success depends on hunting party size. Additionally, the percent of successful hunts is shown on the y-axis.
  \end{QAlist}
  \item What is the explanatory variable?
  \begin{QAlist}
    \item The explanatory variable is the size of the hunting party.
  \end{QAlist}
  \item Does any aspect of this regression concern you (i.e., consider the regression assumptions)?
  \begin{QAlist}
    \item The data appear to be very slightly curved but there is no evidence of a funnel-shape. Thus, the data may be slightly non-linear but they appear homoscedastic.
  \end{QAlist}
  \item In terms of the variables of the problem, what is the equation of the best-fit line?
  \begin{QAlist}
    \item The equation of the best-fit line is \% Success of Hunt = \Sexpr{int} + \Sexpr{slp}*Number of Hunting Party Members.
  \end{QAlist}
  \item Interpret the value of the slope in terms of the variables of the problem.
  \begin{QAlist}
    \item The slope indicates that the percent of successful hunts increases by \Sexpr{slp}, on average, for every increase of one member to the hunting party.
  \end{QAlist}
  \item Interpret the value of the intercept in terms of the variables of the problem.
  \begin{QAlist}
    \item The intercept indicates that the percent of successful hunts is \Sexpr{int}, on average, for hunting parties with no members. This is nonsensical because 0 hunting members is an extrapolation.
  \end{QAlist}
  \item What is the predicted hunt success if the hunting party consists of 20 chimpanzees?
  \begin{QAlist}
    \item The predicted hunt success for parties with 20 individuals is an extrapolation, because 20 is outside the range of number of members observed on the x-axis of the fitted-line plot.
  \end{QAlist}
  \item What is the predicted hunt success if the hunting party consists of 12 chimpanzees?
  \begin{QAlist}
    \item The predicted hunt success for parties with 12 individuals is \Sexpr{int} + \Sexpr{slp}*12 = 68.4\%.
  \end{QAlist}
  \item What is the residual if the hunt success for 10 individuals is 50\%?
  \begin{QAlist}
    \item The residual in this case is $50$-(\Sexpr{int} + \Sexpr{slp}*10) = $50$-\Sexpr{formatC(coef(lm.chimp)[1]+coef(lm.chimp)[2]*10,format="f",digits=1)} = \Sexpr{formatC(50-(coef(lm.chimp)[1]+coef(lm.chimp)[2]*10),format="f",digits=1)}. Therefore, it appears that the success of this hunting party is \Sexpr{formatC(-1*(50-(coef(lm.chimp)[1]+coef(lm.chimp)[2]*10)),format="f",digits=1)}\% lower than average for this size of hunting party.
  \end{QAlist}
  \item What proportion of the variability in hunting success is explained by knowing the size of the hunting party?
  \begin{QAlist}
    \item The proportion of the variability in hunting success that is explained by knowing the size of the hunting party is $r^{2}$=\Sexpr{r2}.
  \end{QAlist}
  \item What is the correlation between hunting success and size of hunting party?
  \begin{QAlist}
    \item The correlation between hunting success and size of hunting party is $r=$\Sexpr{formatC(sqrt(rSquared(lm.chimp)),format="f",digits=2)}. [\emph{Note that this is the square root of $r^{2}$.}]
  \end{QAlist}
  \item How much does hunt success decrease, on average, if there are two fewer individuals in the party?
  \begin{QAlist}
    \item If the hunting party has two fewer members, then the hunting success would decrease by \Sexpr{formatC(2*coef(lm.chimp)[2],format="f",digits=1)}\% (i.e., $-2$*\Sexpr{slp}), on average. [\emph{Note that this is two times the slope, with a negative as it asks about ``fewer'' members.}]
  \end{QAlist}
\end{QAlist}


\subsection*{Car Weight and MPG}
In \modref{chap:BivEDAQuant}, an EDA for the relationship between \var{HMPG} (the highway miles per gallon) and \var{Weight} (lbs) of 93 cars from the 1993 model year was performed. This relationship will be explored further here as an example of a complete regression analysis. In this analysis, the regression output will be examined within the context of answering the twelve typical questions. The results are shown in \figref{fig:CarFit}.

<<CarFit, echo=FALSE, message=FALSE, fig.cap="Fitted line plot of the regression of highway MPG on weight of 93 cars from 1993.", fig.pos="h">>=
cars93 <- read.csv("https://raw.githubusercontent.com/droglenc/NCData/master/93cars.csv")
lm2 <- lm(HMPG~Weight,data=cars93)
slp <- formatC(-1*coef(lm2)[2],format="f",digits=4)
int <- formatC(coef(lm2)[1],format="f",digits=0)
r2 <- formatC(rSquared(lm2),format="f",digits=2)
ggplot(data=cars93,mapping=aes(x=Weight,y=HMPG)) +
  geom_point(pch=21,color="black",fill="darkgray",size=2) +
  geom_smooth(method="lm",se=FALSE,color="red",size=1) +
  ggpubr::stat_regline_equation(aes(label=..eq.label..),
                                label.x.npc=0.55,label.y.npc=1) +
  ggpubr::stat_regline_equation(aes(label=..rr.label..),
                                label.x.npc=0.55,label.y.npc=0.90) +
  labs(x="Weight (lbs)",y="Highway MPG") +
  theme_NCStats() +
  theme(aspect.ratio=1,plot.title=element_text(size=12))
@

\begin{QAlist}
  \item What is the response variable?
  \begin{QAlist}
    \item The response variable in this analysis is the highway MPG, because that is the variable that we are trying to learn about or explain the variability of.
  \end{QAlist}
  \item What is the explanatory variable?
  \begin{QAlist}
    \item The explanatory variable in this analysis is the weight of the car (by process of elimination).
  \end{QAlist}
  \item Do any aspects of this regression fit concern you?
  \begin{QAlist}
    \item The simple linear regression model appears to fit the data moderately well as the fitted-line plot \figrefp{fig:CarFit} shows only a very slight curvature and only very slight heteroscedasticity.\footnote{In advanced statistics books, objective measures for determining whether there is significant curvature or heteroscedasticity in the data are used. In this book, we will only be concerned with whether there is strong evidence of curvature or heteroscedasticity. There does not seem to be either here.}.
  \end{QAlist}
  \item In terms of the variables of the problem, what is the equation of the best-fit line?
  \begin{QAlist}
    \item The equation of the best-fit line for this problem is HMPG = \Sexpr{int} - \Sexpr{slp}Weight.
  \end{QAlist}
  \item Interpret the value of the slope in terms of the variables of the problem.
  \begin{QAlist}
    \item The slope indicates that for every increase of one pound of car weight the highway MPG decreases by \Sexpr{slp}, on average.
  \end{QAlist}
  \item Interpret the value of the intercept in terms of the variables of the problem.
  \begin{QAlist}
    \item The intercept indicates that a car with 0 weight will have a highway MPG value of \Sexpr{int}, on average. [\emph{Note that this is the correct interpretation of the intercept. However, it is nonsensical because it is an extrapolation; i.e., no car will weigh 0 pounds.}]
  \end{QAlist}
  \item What is the predicted highway MPG for a car that weighs 3100 lbs?
  \begin{QAlist}
    \item The predicted highway MPG for a car that weighs 3100 lbs is \Sexpr{int} - \Sexpr{slp}(3100) = 29.4 MPG.
  \end{QAlist}
  \item What is the predicted highway MPG for a car that weighs 5100 lbs?
  \begin{QAlist}
    \item The predicted highway MPG for a car that weighs 5100 lbs should not be computed with the results of this regression, because 5100 lbs is outside the domain of the data \figrefp{fig:CarFit}.
  \end{QAlist}
  \item What is the residual for a car that weights 3500 lbs and has a highway MPG of 24?
  \begin{QAlist}
    \item The predicted highway MPG for a car that weighs 3500 lbs is \Sexpr{int} - \Sexpr{slp}(3500) = 26.5. Thus, the residual for this car is 24 - 26.5 = -2.5. Therefore, it appears that this car gets 2.5 MPG LESS than an average car with the same weight.
  \end{QAlist}
  \item What proportion of the variability in highway MPG is explained by knowing the weight of the car?
  \begin{QAlist}
    \item The proportion of the variability in highway MPG that is explained by knowing the weight of the car is $r^{2}$=\Sexpr{r2}.
  \end{QAlist}
  \item What is the correlation between highway MPG and car weight?
  \begin{QAlist}
    \item The correlation between highway MPG and car weight is $r=$\Sexpr{formatC(-1*sqrt(rSquared(lm2)),format="f",digits=2)}. [\emph{Not that this is the square root of $r^{2}$, but as a negative because form of the relationship between highway MPG and weight is negative.}
  \end{QAlist}
  \item How much is the highway MPG expected to change if a car is 1000 lbs heavier?
  \begin{QAlist}
    \item If the car was 1000 lbs heavier, you would expect the car's highway MPG to decrease by \Sexpr{formatC(-1000*coef(lm2)[2],format="f",digits=2)}. [\emph{Note that this is 1000 slopes.}]
  \end{QAlist}
\end{QAlist}
