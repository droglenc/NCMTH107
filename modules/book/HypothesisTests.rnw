<<echo=FALSE, cache=FALSE, results='hide'>>=
set_parent('IntroStats.Rnw')
@

\chapter{Hypothesis Tests} \label{chap:HypothesisTests}

\vspace*{-24pt}
\minitoc
\vspace*{24pt}

\lettrine{A}{ statistic is an imperfect estimate} of a parameter because of sampling variability. There are two calculations using the results of a single sample that recognize this imperfection and allow conclusions to be made about a parameter. First, a researcher may form an \emph{a priori} hypothesis about a parameter and then use the information in the sample to make a judgment about the ``correctness'' of that hypothesis. Second, a researcher may form, from the information in the sample, a range of values that is likely to contain the parameter. The first method is called \emph{hypothesis testing} and is the subject of this module. The second method consists of constructing a \emph{confidence region}, which is introduced in \modref{chap:ConfidenceRegions}. Specific applications of these two techniques are described in Modules \ref{chap:ZTest}-\ref{chap:GOF}.


\section{Hypothesis Testing \& The Scientific Method} \label{sect:SciMethod}
In its simplest form, the scientific method has four steps:

\vspace*{-6pt}
\begin{Enumerate}
  \item Observe and describe a natural phenomenon.
  \item Formulate a hypothesis to explain the phenomenon.
  \item Use the hypothesis to predict new observations.
  \item Experimentally test the predictions.
\end{Enumerate}
\vspace*{-6pt}

If the results of the experiment do not match the predictions, then the hypothesis is rejected and an alternative hypothesis is proposed. If the results of the experiment closely match the predictions, then belief in the hypothesis is gained, though the hypothesis will likely be subjected to further experimentation.

Statistical hypothesis testing is key to using the scientific method in many fields of study and, in fact, closely follows the scientific method in concept. Statistical hypothesis testing begins by formulating two competing statistical hypotheses from a research hypothesis. One of these hypotheses (the null) is used to predict the parameter of interest. Data is then collected and statistical methods are used to determine whether the observed statistic closely matches the prediction made from the null hypothesis or not. Probability \modrefp{chap:ProbIntro} is used to measure the degree of matching with sampling variability taken into account. This process and the theory underlying statistical hypothesis testing is explained in detail in this module.


\section{Statistical Hypotheses} \label{sec:Hypotheses}
Hypotheses are classified into two types: (1) research hypothesis and (2) statistical hypotheses. A research hypothesis is a ``wordy'' statement about the question or phenomenon that the researcher is testing. Four example research hypotheses are:

\vspace*{-8pt}
\begin{Enumerate}
  \item A medical researcher is concerned that a new medicine may change patients' mean pulse rate (from the ``known'' mean pulse rate of 82 bpm for individuals in the study population not using the new medicine).
   \item A chemist has invented an additive to car batteries that she thinks will extend the current 36 month average life of a battery.
  \item An engineer wants to determine if a new type of insulation will reduce the average heating costs of a typical house (which are currently \$145 per month).
  \item A researcher is concerned whether, on average, Alzheimer's caregivers at a particular facility are clinically depressed (as suggested by a mean Beck Depression Inventory (BDI) score greater than 25)
\end{Enumerate}
\vspace*{-8pt}

Research hypotheses are converted to statistical hypotheses that are mathematical and more easily subjected to statistical methods. There are two types of statistical hypotheses: (1) the null hypothesis and (2) the alternative hypothesis. The \textbf{null hypothesis}, abbreviated as $H_{0}$, is a specific statement of no difference between a parameter and a specific value or between two parameters. The $H_{0}$ ALWAYS contains an equals sign because it always represents ``no difference.''  The \textbf{alternative hypothesis}, abbreviated as $H_{A}$, always states that there is some sort of difference between a parameter and a specific value or between two parameters. The type of difference comes from the research hypothesis and will require use of a less than (\verb"<"), greater than (\verb">"), or not equals ($\neq$) sign. Null and alternative hypotheses that correspond to the four research hypotheses above are:

\vspace*{-8pt}
\begin{Enumerate}
  \item $H_{A}:\mu\neq82$ and $H_{0}:\mu=82$ (where $\mu$ represents the mean pulse rate for individuals in the study population that take the new medicine; thus, the alternative hypothesis represents a change from the ``normal'' pulse rate).
 \item $H_{A}:\mu>36$ and $H_{0}:\mu=36$ (where $\mu$ represents the mean life of batteries with the new additive; thus, this alternative hypothesis represents an extension of the current battery life).
  \item $H_{A}:\mu<145$ and $H_{0}:\mu=145$ (where $\mu$ represents the mean monthly heating bill for houses that receive the new type of insulation; thus, this alternative hypothesis represents a decline in heating bills from the previous ``normal'' amount).
 \item $H_{A}:\mu>25$ and $H_{0}:\mu=25$ (where $\mu$ represents the mean BDI score; thus, this alternative hypothesis represents a mean score that indicates clinical depression).
\end{Enumerate}
\vspace*{-8pt}

The sign used in the alternative hypothesis comes directly from the wording of the research hypothesis \tabrefp{tab:HAwords}. An alternative hypothesis that contains the $\neq$ sign is called a \textbf{two-tailed alternative}, as the value can be ``not equal'' to another value in two ways; i.e., less than or greater than. Alternative hypotheses with the $<$ or the $>$ signs are called \textbf{one-tailed alternatives}. The null hypothesis is easily constructed from the alternative hypothesis by replacing the sign in the alternative hypothesis with an equals sign.

\begin{table}[htbp]
  \caption{Common words that indicate which sign to use in the alternative hypothesis.}
  \label{tab:HAwords}
  \centering
  \begin{tabular}{ccc}
\hline\hline
$>$ & $<$ & $\neq$ \\
\hline
is greater than & is less than & is not equal to \\
is more than & is below & is different from \\
is larger than & is lower than & has changed from \\
is longer than & is shorter than & is not the same as \\
is bigger than & is smaller than &  \\
is better than & is reduced from &  \\
is at least & is at most &  \\
is not less than & is not more than &  \\
\hline\hline
  \end{tabular}
\end{table}


\subsection{Hypothesis Testing Concept}
\vspace{-6pt}
Statistical hypothesis testing begins by using the null hypothesis to predict what value one should expect for the mean in a sample. So, for the Square Lake example (from \modref{chap:WhyStatsImportant}), if $H_{0}:\mu=105$ and $H_{A}:\mu<105$, then one would expect, if the null hypothesis is true, that the observed sample mean would be 105. If the observed sample mean was NOT equal to 105 and sampling variability did not exist, then the prediction based on the null hypothesis would not be supported and one would conclude that the null hypothesis was incorrect. In other words, one would conclude that the population mean was not equal to 105.

Of course, sampling variability does exist and it complicates matters. The simple interpretation of not supporting $H_{0}$ because the observed sample mean did not equal the hypothesized population mean canNOT be made because, with sampling variability, one would not expect a statistic to exactly equal the parameter in the population from which the sample was extracted. For example, even if the null hypothesis was correct, one would not expect, with sampling variability, the observed sample mean to exactly equal 105; rather, one would expect the observed sample mean to be \textbf{reasonably} close to 105.

Thus, hypothesis testing is a process to determine if the difference between the observed statistic and the expected statistic based on the null hypothesis is ``large'' \textbf{relative to sampling variability}. For example, the standard error of $\bar{x}$ for samples of $n=50$ in the Square Lake example is $\frac{\sigma}{\sqrt{n}}=$$\frac{31.5}{\sqrt{50}}$$=4.45$. With this sampling variability, an observed sample mean of 103 would be considered reasonably close to 105 and one would have more belief in $H_{0}:\mu=105$ \figrefp{fig:HOSLExample}. However, an observed sample mean of 90 is further away from 105 than one would expect based on sampling variability alone and belief in $H_{0}:\mu=105$ would lessen \figrefp{fig:HOSLExample}.

<<HOSLExample, echo=FALSE, fig.width=5.5, out.width='.53\\linewidth', fig.cap="Sampling distribution of samples means of n=50 from the Square Lake population ASSUMING that $\\mu$=105.">>=
par(mar=c(3,1.5,0.5,1.5),mgp=c(1.9,0.4,0),yaxs="i",tcl=-0.2,xpd=TRUE)
mu <- 105
SE <- 31.5/sqrt(50)
x0 <- seq(mu-5*SE,mu+5*SE,length.out=200)
norm0 <- dnorm(x0,mean=mu,sd=SE)
plot(x0,norm0,type="l",lwd=3,bty="n",
     xlab="Mean Total Length (mm)",
     ylab="",yaxt="n",xaxt="n")
axis(1,round(mu+(-3:3)*SE,0))
axis(1,round(mu+c(0,3)*SE,0))
abline(h=0)
arrows(103,0.01,103,0,length=0.15,col="blue2",lwd=2)
arrows(103,0.01,112,0.06,code=0,col="blue2",lwd=2)
text(118.8,0.07,"Given sampling variability;\na mean of 103 is likely\nMORE belief in Ho",col="blue2",font=2,xpd=TRUE)
arrows(90,0.06,90,0,length=0.15,col="darkred",lwd=2)
text(91,0.06,"Given sampling variability;\na mean of 90 is UNlikely\nLESS belief in Ho",col="darkred",font=2,pos=3,xpd=TRUE)
@

While the above procedure is intuitively appealing, the conclusions are not as clear when the examples chosen (i.e., samples means of 103 and 90) are not as extremely close or distant from the null hypothesized value. For example, what would one concludeif the observed sample mean was 97?. A first step in creating a more objective decision criteria is to compute the ``p-value.'' A p-value is the probability of the observed statistic or a value of the statistic more extreme assuming that the null hypothesis is true. The p-value is described in more detail below given its centrality to making conclusions about statistical hypotheses.

The meaning of the phrase ``or more extreme'' in the p-value definition is derived from the sign in $H_{A}$ \figrefp{fig:HOtails}. If $H_{A}$ is the ``less than'' situation, then ``or more extreme'' means ``less than'' or ``shade to the left'' for the probability calculation. The ``greater than'' situation is defined similarly but would result in shading to the ``right.''  In the ``not equals'' situation, ``or more extreme'' means further into the tail AND the exact same size of tail on the other side of the distribution. It is clear from \figref{fig:HOtails} why ``less than'' and ``greater than'' are one-tailed alternatives and ``not equals'' is a two-tailed alternative.

<<HOtails, echo=FALSE, fig.width=6, fig.height=2, out.width='.8\\linewidth', fig.cap="Depiction of ``or more extreme'' (red areas) in p-values for the three possible alternative hypotheses.">>=
par(mfcol=c(1,3),mar=c(2,0,2,0),las=1,tcl=-0.2)
x0 <- seq(-4,4,by=0.001)
norm0 <- dnorm(x0,0,1)
plot(x0,norm0,type="l",xlab="",ylab="",axes=FALSE,lwd=3)
mtext(expression(bold(H[A]:mu<mu[0])),3)
cv1 <- -1.5
xc1 <- c(cv1,x0[x0<=cv1],cv1)
yc1 <- c(0,norm0[x0<=cv1],0)
polygon(xc1,yc1,col="red",border="red")
lines(x0,norm0,lwd=3)
text(cv1,-0.03,expression(bar(x)),cex=2,xpd=TRUE)

plot(x0,norm0,type="l",xlab="",ylab="",axes=FALSE,lwd=3)
mtext(expression(bold(H[A]:mu>mu[0])),3)
cv2 <- 1.5
xc2 <- c(cv2,x0[x0>=cv2],cv2)
yc2 <- c(0,norm0[x0>=cv2],0)
polygon(xc2,yc2,col="red",border="red")
lines(x0,norm0,lwd=3)
text(cv2,-0.03,expression(bar(x)),cex=2,xpd=TRUE)

plot(x0,norm0,type="l",xlab="",ylab="",axes=FALSE,lwd=3)
mtext(expression(bold(H[A]:mu!=mu[0])),3)
polygon(xc1,yc1,col="red",border="red")
polygon(xc2,yc2,col="red",border="red")
lines(x0,norm0,lwd=3)
text(cv2,-0.03,expression(bar(x)),cex=2,xpd=TRUE)
@

The ``assuming that the null hypothesis is true'' phrase is used to define a $\mu$ for the sampling distribution on which the p-value will be calculated. This sampling distribution is called the \textbf{null distribution} because it depends on the value of $\mu$ from the null hypothesis. One must remember that the null distribution represents the distribution of all possible sample means assuming that the null hypothesis is true; it does NOT represent the actual sample means.\footnote{Of course, unless the null hypothesis happens to be perfectly true.}  The null distribution in the Square Lake example is thus $\bar{x}\sim N(105,4.45)$ because $n=50>30$ (so the Central Limit Theorem holds), $H_{0}:\mu=105$, and SE=$\frac{31.49}{\sqrt{50}}$=$4.45$.

The p-value is computed with a ``forward'' normal distribution calculation on the null sampling distribution. For example, suppose that a sample mean of 100 was observed with $n=50$ from Square Lake (as it was in \tabref{tab:SquareLakeSample1}). The p-value in this case would be ``the probability of observing $\bar{x}=100$ or a smaller value assuming that $\mu=105$.''  This probability is computed by finding the area to the left of 100 on a $N(105,4.45)$ null distribution and is the exact same type of calculation as that made in \sectref{sect:sdprob}. Thus, this p-value of \Sexpr{kPvalue(pnorm(100,mean=105,sd=31.49/sqrt(50)))} is computed as below and shown in \figref{fig:SLpvalue1}.

<<SLpvalue1, par1a=TRUE, fig.cap="Depiction of the p-value for the Square Lake example where $\\bar{x}=100$ and $H_{A}:\\mu<105$.">>=
( distrib(100,mean=105,sd=31.49/sqrt(50)) )
@

Interpreting the p-value requires critically thinking about the p-value definition and how it is calculated. Small p-values appear when the observed statistic is ``far'' from the null hypothesized value. In this case there is a small probability of seeing the observed statistic ASSUMING that $H_{0}$ is true. Thus, the assumption is likely wrong and $H_{0}$ is likely incorrect. In contrast, large p-values appear when the observed statistic is close to the null hypothesized value suggesting that the assumption about $H_{0}$ may be correct.

The p-value serves as a numerical measure on which to base a conclusion about $H_{0}$. To do this objectively requires an objective definition of what it means to be a ``small'' or ``large'' p-value. Statisticians use a cut-off value, called the rejection criterion and symbolized with $\alpha$, such that p-values less than $\alpha$ are considered small and would result in rejecting $H_{0}$ as a viable hypothesis. The value of $\alpha$ is typically small, usually set at $0.05$, although $\alpha=0.01$ and $\alpha=0.10$ are also commonly used.

The choice of $\alpha$ is made by the person conducting the hypothesis test and is based on how much evidence a researcher demands before rejecting $H_{0}$. Smaller values of $\alpha$ require a larger difference between the observed statistic and the null hypothesized value and, thus, require ``more evidence'' of a difference for the $H_{0}$ to be rejected. For example, if rejection of the null hypothesis will be heavily scrutinized by regulatory agencies, then the researcher may want to be very sure before claiming a difference and should then set $\alpha$ at a smaller value, say $\alpha=0.01$. The actual choice for $\alpha$ MUST be made before collecting any data and canNOT be changed once the data has been collected. In other words, once the data are in hand, a researcher cannot lower or raise $\alpha$ to achieve a desired outcome regarding $H_{0}$.

\warn{The value of the rejection criterion ($\alpha$) is set by the researcher BEFORE data is collected.}

The null hypothesis in the Square Lake example is not rejected because the p-value (i.e., \Sexpr{kPvalue(pnorm(100,mean=105,sd=31.49/sqrt(50)),include.p=FALSE)}) is larger than any of the common values of $\alpha$. Thus, the conclusion in this example is that it is possible that the mean of the entire population is equal to 105 and it is not likely that the population mean is less than 105. In other words, observing a sample mean of 100 is likely to happen based on random sampling variability alone and it is unlikely that the null hypothesized value is incorrect.


\newpage
\section{Test Statistics and Effect Sizes}
Instead of reporting the observed statistic and the resulting p-value, it may be of interest to know how ``far'' the observed statistic was from the hypothesized value of the parameter. This is easily calculated with
\[ \text{Observed Statistic}-\text{Hypothesized Parameter} \]
where ``Hypothesized Parameter'' represents the specific value in $H_{0}$. However, the meaning of this difference is difficult to interpret without an understanding of the standard error of the statistic. For example, a difference of 10 between the observed statistic and the hypothesized parameter seems ``very different'' if the standard error is 3 but does not seem ``different'' if the standard error is 15 \figrefp{fig:EffectSizeSE}.

<<EffectSizeSE, echo=FALSE, fig.width=5.5, out.width='.53\\linewidth', fig.cap="Sampling distribution of samples means with SE=3 (Left) and SE=15 (Right). A single observed sample mean of 90 (a difference of 10 from the hypothesized mean of 100) is shown by the red dot and arrow.">>=
par(mfrow=c(1,2),mar=c(3,1.5,1.5,1.5),mgp=c(1.9,0.4,0),yaxs="i",tcl=-0.2)
mu <- 100
xbar <- 90
SE <- c(3,15)
zext <- 3
x0 <- seq(mu-zext*SE[2],mu+zext*SE[2],length.out=200)
norm0 <- dnorm(x0,mean=mu,sd=SE[1])
norm1 <- dnorm(x0,mean=mu,sd=SE[2])
xlim <- range(x0)
plot(x0,norm0,type="l",lwd=3,bty="n",xaxt="n",yaxt="n",xlim=xlim,
     xlab="Sample Means",ylab="")
points(xbar,0.0005,col="red",pch=16,bg="red",xpd=TRUE,cex=1.25)
axis(1,c(60,80,120,140))
axis(1,100,col.ticks="blue2",col.axis="blue2",tcl=-0.3)
rect(50,0.084,150,0.116,col="white",border=NA)
text(100,0.1,"A difference of 10\nappears MORE\nextreme with SE=3",
     col="darkred",font=2)
arrows(90,0.04,90,0.001,col="red",length=0.15,angle=15)

plot(x0,norm1,type="l",lwd=3,bty="n",xaxt="n",yaxt="n",xlim=xlim,
     xlab="Sample Means",ylab="")
points(xbar,0.0005,col="red",pch=16,bg="red",xpd=TRUE,cex=1.25)
axis(1,c(60,80,120,140))
axis(1,100,col.ticks="blue2",col.axis="blue2",tcl=-0.3)
rect(50,0.017,150,0.0229,col="white",border=NA)
text(100,0.02,"A difference of 10\nappears LESS\nextreme with SE=15",
     col="blue3",font=2)
arrows(90,0.008,90,0.0007,col="red",length=0.15,angle=15)
@

The difference between the observed statistic and the hypothesized parameter is  standardized to a common scale by dividing by the standard error of the statistic. The result is called a \emph{test statistic} and is generalized with

\begin{equation}  \label{eqn:zTestStatGeneral}
  \text{Test Statistic} = \frac{\text{Observed Statistic}-\text{Hypothesized Parameter}}{SE_{\text{Statistic}}}
\end{equation}

Thus, the test statistic \eqref{eqn:zTestStatGeneral} measures how many standard errors the observed statistic is away from the hypothesized parameter. A relatively large value is indicative of a difference that is likely not due to randomness (i.e., sampling variability) and suggests that the null hypothesis should be rejected.

The test statistic in the Square Lake Example is $\frac{100-105}{\frac{31.49}{\sqrt{50}}}$=$-1.12$. Thus, the observed mean total length of 100 mm is 1.12 standard errors below the null hypothesized mean of 105 mm. From our experience, a little over one SE from the mean is not ``extreme'' and, thus, it is not surprising that the null hypothesis was not rejected.

There are other forms for calculating test statistics, but all test statistics retain the general idea of scaling the difference between what was observed and what was expected from the null hypothesis in terms of sampling variability. Even though there is a one-to-one relationship between a test statistic and a p-value, a test statistic is often reported with a hypothesis test to give another feel for the magnitude of the difference between what was observed and what was predicted.


\section{Hypothesis Testing Concept Summary}

In summary, hypotheses are statistically examined with the following procedure.
\vspace{-8pt}
\begin{Enumerate}
  \item Construct null and alternative hypotheses from the research hypothesis.
  \item Construct an expected value of the statistic based on the null hypothesis (i.e., assume that the null hypothesis is true).
  \item Calculate an observed statistic from the individuals in a sample.
  \item Compare the difference between the observed statistic and the expected statistic based on the null hypothesis in relation to sampling variability (i.e., calculate a test statistic and p-value).
  \item Use the p-value to determine if this difference is ``large'' or not.
  \begin{Itemize}
    \item If this difference is ``large'' (i.e., p-value$<\alpha$), then reject the null hypothesis.
    \item If this difference is not ``large'' (i.e., p-value$>\alpha$), then ``Do Not Reject'' the null hypothesis.
  \end{Itemize}
\end{Enumerate}

Statisticians say ``do not reject H$_{0}$'' rather than ``accept H$_{0}$ as true'' when the p-value $>\alpha$ for two reasons. First, there are several other possible values, besides the specific value in the null hypothesis, that would lead to ``do not reject'' conclusions. For example, if a null hypothesized value of 105 was not rejected, then values of 104.99, 104.98, etc. would also likely not be rejected.\footnote{In fact, for example, the values in a 95\% confidence interval -- see \modref{chap:ConfidenceRegions} -- represent all possible hypothesized values that would not be rejected with a two-tailed $H_{A}$ using $\alpha=0.05$.}  So, we don't say that we ``accept'' a particular hypothesized value when we know many other values would also be ``accepted.''

Second, the null hypothesis is almost always not true. Consider the null hypothesis of the Square Lake example (i.e., ``that the mean length is 105 mm''). The mean length of fish in Square Lake is undoubtedly not exactly equal to 105. It may be 104.9, 105.01, or some other more disparate value. The point is that the specific value of the hypothesis is likely never true, especially for a continuous variable. The problem is that it takes large amounts of data to be able to distinguish means that are very close to the true population mean (i.e., it is difficult to distinguish between 104.9 and 105 when sampling variability is present). Very often we will not take a sample size large enough to distinguish these subtle differences. Thus, we will say that we ``do not reject H$_{0}$'' because there simply was not enough data to reject it.
